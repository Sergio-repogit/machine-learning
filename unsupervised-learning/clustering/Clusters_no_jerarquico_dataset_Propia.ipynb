{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d605bf",
   "metadata": {},
   "source": [
    "### Clusters no Jer√°rquicos\n",
    "\n",
    "Una liga internacional de **e-sports** quiere segmentar a sus equipos profesionales para dise√±ar programas de entrenamiento personalizados, estrategias de balanceo competitivo (emparejamientos, divisiones, etc.) y estrategias de contenido y retransmisi√≥n en plataformas de streaming.\n",
    "\n",
    "Para ello se ha recopilado informaci√≥n de **$18$ equipos profesionales** de un mismo videojuego competitivo. Las variables disponibles son:\n",
    "\n",
    "+ `Equipo`: identificador del equipo.\n",
    "\n",
    "+ `Win_rate`: porcentaje de partidas ganadas en la √∫ltima temporada (%).\n",
    "\n",
    "+ `KDA`: ratio medio de bajas/asistencias frente a muertes (Kill/Death/Assist ratio).\n",
    "\n",
    "+ `Control_obj`: porcentaje de control de objetivos clave del mapa (torres, dragones, puntos de captura, etc.) (%).\n",
    "\n",
    "+ `Entreno_horas`: horas medias de entrenamiento en equipo por semana.\n",
    "\n",
    "El **objetivo** es aplicar t√©cnicas de clustering no jer√°rquico para identificar perfiles de equipos (por ejemplo, equipos hiperentrenados, equipos muy eficientes en objetivos, equipos dependientes del ‚Äúearly game‚Äù, etc.). La base de datos se resume en la siguiente tabla:\n",
    "\n",
    "| Equipo  | Win_rate | KDA  | Control_obj | Entreno_horas |\n",
    "| ------- | -------- | ---- | ----------- | ------------- |\n",
    "| Team_01 | 69.6     | 3.93 | 66.0        | 31            |\n",
    "| Team_02 | 66.5     | 4.21 | 49.9        | 30            |\n",
    "| Team_03 | 54.7     | 2.43 | 71.2        | 18            |\n",
    "| Team_04 | 49.1     | 3.69 | 63.4        | 24            |\n",
    "| Team_05 | 57.9     | 4.20 | 30.1        | 32            |\n",
    "| Team_06 | 54.2     | 3.55 | 54.7        | 24            |\n",
    "| Team_07 | 67.4     | 2.92 | 73.4        | 37            |\n",
    "| Team_08 | 50.6     | 1.80 | 42.2        | 18            |\n",
    "| Team_09 | 56.7     | 2.80 | 46.3        | 32            |\n",
    "| Team_10 | 60.4     | 3.33 | 73.5        | 25            |\n",
    "| Team_11 | 71.8     | 4.24 | 39.6        | 32            |\n",
    "| Team_12 | 57.7     | 4.40 | 58.4        | 21            |\n",
    "| Team_13 | 49.9     | 2.93 | 41.9        | 40            |\n",
    "| Team_14 | 66.5     | 4.10 | 78.4        | 34            |\n",
    "| Team_15 | 61.6     | 2.28 | 70.2        | 32            |\n",
    "| Team_16 | 48.8     | 3.92 | 52.4        | 33            |\n",
    "| Team_17 | 71.8     | 3.15 | 34.0        | 24            |\n",
    "| Team_18 | 74.4     | 1.54 | 46.0        | 29            |\n",
    "\n",
    "Realizar los siguientes apartados:\n",
    "\n",
    "1. Crear la base de datos a partir de dicha informaci√≥n y realizar el an√°lisis exploratorio.\n",
    "2. Realizar el an√°lisis de la existencia de relaci√≥n y semejanza entre las variables.\n",
    "3. Calcular la matriz de proximidad utilizando la distancia euclidiana.\n",
    "4. Estimar e interpretar los modelos de clustering no jer√°rquicos considerando los cuatro m√©todos.\n",
    "5. Calcular el n√∫mero de cl√∫sters √≥ptimos para cada uno de los cuatro m√©todos a partir de los tres criterios vistos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee9229",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5DADE2\"><b>Apartado 1</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ff2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el data frame con los datos indicados\n",
    "wdata <- data.frame(\n",
    "  Equipo = paste0(\"Team_\", sprintf(\"%02d\", 1:18)),\n",
    "  Win_rate = c(69.6, 66.5, 54.7, 49.1, 57.9, 54.2, 67.4, 50.6, 56.7,\n",
    "               60.4, 71.8, 57.7, 49.9, 66.5, 61.6, 48.8, 71.8, 74.4),\n",
    "  KDA = c(3.93, 4.21, 2.43, 3.69, 4.20, 3.55, 2.92, 1.80, 2.80,\n",
    "          3.33, 4.24, 4.40, 2.93, 4.10, 2.28, 3.92, 3.15, 1.54),\n",
    "  Control_obj = c(66.0, 49.9, 71.2, 63.4, 30.1, 54.7, 73.4, 42.2, 46.3,\n",
    "                  73.5, 39.6, 58.4, 41.9, 78.4, 70.2, 52.4, 34.0, 46.0),\n",
    "  Entreno_horas = c(31, 30, 18, 24, 32, 24, 37, 18, 32,\n",
    "                    25, 32, 21, 40, 34, 32, 33, 24, 29)\n",
    ")\n",
    "\n",
    "wdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el an√°lisis exploratorio de las variables utilizadas\n",
    "summary(wdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar librer√≠a necesaria para kurtosis\n",
    "if(!require(moments, quietly = TRUE)){\n",
    "  install.packages(\"moments\", repos = \"https://cloud.r-project.org\")\n",
    "  library(moments)\n",
    "}\n",
    "\n",
    "# Desviaci√≥n est√°ndar de todas las variables num√©ricas\n",
    "sapply(wdata, sd, na.rm = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curtosis de todas las variables num√©ricas\n",
    "sapply(wdata[sapply(wdata, is.numeric)], kurtosis, na.rm = TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797924b",
   "metadata": {},
   "source": [
    "**<u>Interpretaci√≥n resultados</u>.-**\n",
    "\n",
    "La base de datos contiene informaci√≥n de **18 equipos profesionales de e-sports** y cinco variables de las cuales cuatro cuantitativas que describen su rendimiento competitivo y su volumen de entrenamiento, sin presencia de valores at√≠picos ni registros an√≥malos. A continuaci√≥n se interpretan los estad√≠sticos descriptivos de cada variable en su contexto.\n",
    "\n",
    "**Equipo** act√∫a como identificador √∫nico, con 18 equipos diferentes incluidos en el an√°lisis.\n",
    "\n",
    "**Win_rate** recoge el porcentaje de victorias de cada equipo durante la √∫ltima temporada. Presenta valores entre 48.8 % y 74.4 %, con una media aproximada del 60.5 % y una mediana ligeramente menor (59.15 %). La desviaci√≥n est√°ndar (~ 8.51 pp) indica una dispersi√≥n moderada: existen diferencias apreciables entre equipos m√°s y menos consistentes. Su curtosis (~ 1.69) sugiere una distribuci√≥n con valores concentrados en un rango relativamente homog√©neo. En t√©rminos competitivos, los equipos muestran variabilidad en desempe√±o, aunque sin extremos de dominio absoluto ni bajo rendimiento cr√≠tico.\n",
    "\n",
    "**KDA** mide el equilibrio entre eliminaciones/asistencias y muertes. Se sit√∫a entre 1.54 y 4.40, con media de 3.30 y mediana de 3.44, lo que apunta a una distribuci√≥n bastante sim√©trica. La desviaci√≥n est√°ndar (~ 0.88) refleja diferencias moderadas en eficiencia durante las partidas. La curtosis (~ 2.19) indica que la distribuci√≥n es ligeramente platic√∫rtica, sin colas pesadas. En t√©rminos de juego, los equipos muestran distintos estilos y niveles de eficacia mec√°nica, pero sin valores extremadamente bajos ni excepcionalmente altos.\n",
    "\n",
    "**Control_obj**, porcentaje de control de objetivos del mapa, presenta el rango m√°s amplio del conjunto (30.1 %‚Äì78.4 %). La media (55.1 %) y la mediana (53.55 %) sugieren un desempe√±o medio-alto en la mayor√≠a de equipos. Sin embargo, la desviaci√≥n est√°ndar (~ 14.85) evidencia alta heterogeneidad: algunos equipos dominan con claridad los objetivos, mientras que otros tienen dificultades para competir en esta faceta t√°ctica. Su curtosis (~ 1.77) confirma la ausencia de colas marcadas y apunta a una distribuci√≥n m√°s aplanada que la normal.\n",
    "\n",
    "Por √∫ltimo, **Entreno_horas**, las horas semanales de entrenamiento conjunto, oscila entre 18 y 40 h. La media (28.7 h) y la mediana (30.5 h) indican que la mayor√≠a de equipos se sit√∫an alrededor de las 30 h semanales. La desviaci√≥n est√°ndar (~ 6.24 h) revela diferencias notables en la carga de trabajo, posiblemente asociadas a filosof√≠as de entrenamiento o disponibilidad de recursos. Su curtosis (~ 2.22) muestra de nuevo una distribuci√≥n relativamente plana sin valores extremos.\n",
    "\n",
    "En conjunto, las variables presentan **variabilidad moderada y ausencia de valores extremos**, lo que indica un comportamiento estad√≠stico estable en el que todos los equipos se sit√∫an en rangos razonables de rendimiento y dedicaci√≥n. Esto proporciona una base adecuada para aplicar m√©todos de clustering, ya que las diferencias observadas pueden ayudar a identificar perfiles competitivos diferenciados sin verse distorsionados por outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c00ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las variables num√©ricas\n",
    "numeric_vars <- wdata[, sapply(wdata, is.numeric)]\n",
    "\n",
    "# ---------- BOXPLOTS AUTOM√ÅTICOS ----------\n",
    "par(mfrow = c(2, ceiling(ncol(numeric_vars)/2)))\n",
    "for(i in 1:ncol(numeric_vars)){\n",
    "  boxplot(numeric_vars[, i], main = colnames(numeric_vars)[i])\n",
    "}\n",
    "\n",
    "# ---------- HISTOGRAMAS AUTOM√ÅTICOS ----------\n",
    "par(mfrow = c(2, ceiling(ncol(numeric_vars)/2)))\n",
    "for(i in 1:ncol(numeric_vars)){\n",
    "  hist(numeric_vars[, i], main = colnames(numeric_vars)[i], xlab = \"\")\n",
    "}\n",
    "\n",
    "# Volver a la configuraci√≥n normal\n",
    "par(mfrow = c(1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7471d1a",
   "metadata": {},
   "source": [
    "**<u>Interpretaci√≥n resultados</u>.‚Äì**\n",
    "\n",
    "Los boxplots y los histogramas permiten comparar la **distribuci√≥n**, **variabilidad** y **diferencias estructurales** entre las cuatro variables num√©ricas: *Win_rate*, *KDA*, *Control_obj* y *Entreno_horas*. A continuaci√≥n se presenta un an√°lisis comparativo conjunto.\n",
    "\n",
    "**Comparaci√≥n de dispersi√≥n y variabilidad**\n",
    "\n",
    "A partir de los **boxplots**, se observa que ninguna variable presenta valores at√≠picos visibles, lo que indica estabilidad en las observaciones. Sin embargo, las variables difieren en su grado de dispersi√≥n:\n",
    "\n",
    "- **Win_rate** y **Control_obj** son las m√©tricas con **mayor variabilidad**. Los rangos son amplios y abarcan equipos con rendimientos bajos hasta equipos significativamente fuertes. Esto refleja heterogeneidad competitiva, tanto en victorias como en control de objetivos.\n",
    "- **KDA** y **Entreno_horas** muestran **menor dispersi√≥n**, con valores m√°s concentrados en intervalos estrechos. Esto sugiere comportamientos m√°s homog√©neos entre equipos en la eficiencia individual (KDA) y en el volumen semanal de entrenamiento.\n",
    "\n",
    "**Comparaci√≥n de la forma de las distribuciones**\n",
    "\n",
    "Los **histogramas** permiten contrastar c√≥mo se distribuye la informaci√≥n dentro de cada variable:\n",
    "\n",
    "- **Win_rate** presenta una distribuci√≥n relativamente uniforme, sin acumulaciones muy marcadas, lo que indica variedad equilibrada en rendimiento general.\n",
    "- **KDA** muestra una ligera tendencia hacia valores medios-altos, lo que sugiere que la mayor√≠a de equipos mantienen ratios de eficiencia razonablemente altos.\n",
    "- **Control_obj** exhibe dos zonas destacadas: un grupo de equipos con control moderado y otro con valores altos, reflejando diferencias estrat√©gicas en la toma de objetivos.\n",
    "- **Entreno_horas** tiene una distribuci√≥n algo sesgada hacia valores medios y altos, indicando que la mayor√≠a de equipos entrenan entre 25 y 35 horas semanales, con pocos en los extremos.\n",
    "\n",
    " **Conclusi√≥n**\n",
    "\n",
    "Las cuatro variables difieren significativamente en su estructura interna. Mientras que *Win_rate* y *Control_obj* presentan mayor heterogeneidad indicando diferencias claras en nivel competitivo y control estrat√©gico del mapa variables como *KDA* y *Entreno_horas* son m√°s homog√©neas, lo que sugiere pr√°cticas y niveles de eficiencia m√°s similares entre los equipos. Esta comparaci√≥n evidencia que la competitividad no est√° determinada solo por la pr√°ctica o la eficiencia individual, sino por diferencias estructurales m√°s amplias entre equipos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2332c2",
   "metadata": {},
   "source": [
    "Para el an√°lisis posterior es necesario normalizar las variables num√©ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8acca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo variables num√©ricas\n",
    "num_vars <- wdata[, sapply(wdata, is.numeric)]\n",
    "\n",
    "# Tipificar (escalado est√°ndar)\n",
    "datos <- as.data.frame(scale(num_vars, center = TRUE, scale = TRUE))\n",
    "\n",
    "# Si quieres mantener tambi√©n el nombre del municipio:\n",
    "datos <- cbind(Equipo = wdata$Equipo,\n",
    "                           datos)\n",
    "\n",
    "datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1326f3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5DADE2\"><b>Apartado 2</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10288c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos informaci√≥n gr√°fica sobre la relaci√≥n de las variables\n",
    "# Instalamos la librer√≠a GGally\n",
    "install.packages(\"GGally\")\n",
    "install.packages(\"dplyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e785d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la librer√≠a\n",
    "library(GGally)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo variables num√©ricas\n",
    "datos_est <- datos %>% select(where(is.numeric))\n",
    "\n",
    "# ggpairs solo con columnas num√©ricas\n",
    "ggpairs(datos_est) +\n",
    "  theme_minimal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025eff1d",
   "metadata": {},
   "source": [
    "**<u>Interpretaci√≥n resultados</u>.-**  \n",
    "La matriz de gr√°ficos permite analizar la estructura de relaci√≥n entre las variables estandarizadas vinculadas al rendimiento y comportamiento del jugador: *Win_rate*, *KDA*, *Control_obj* y *Entreno_horas*. En t√©rminos generales, **las correlaciones son muy bajas**, lo que indica que las variables aportan informaci√≥n bastante independiente entre s√≠ y, por tanto, pueden resultar √∫tiles en procesos de clustering sin riesgo de redundancia excesiva.\n",
    "\n",
    "En primer lugar, la relaci√≥n entre **Entreno_horas y Win_rate** muestra la correlaci√≥n m√°s destacada (~ 0.21), aunque sigue siendo d√©bil. Esta tendencia sugiere que **un mayor tiempo de entrenamiento podr√≠a asociarse ligeramente con un mayor porcentaje de victorias**, pero la dispersi√≥n observada revela que no existe un patr√≥n lineal claro. Del mismo modo, la correlaci√≥n entre **Entreno_horas y KDA** (~ 0.19) apunta a que un mayor volumen de pr√°ctica podr√≠a asociarse con un desempe√±o algo m√°s eficiente en t√©rminos de bajas y asistencias, aunque de forma modesta.\n",
    "\n",
    "Por otro lado, las variables **KDA** y **Win_rate** presentan una correlaci√≥n pr√°cticamente nula, lo cual indica que **tener buenos valores de KDA no garantiza necesariamente un mayor porcentaje de victorias**, probablemente por factores estrat√©gicos o por el rol desempe√±ado dentro del equipo. De igual modo, **Control_obj** no muestra relaciones lineales evidentes con ninguna de las otras m√©tricas (todas cercanas a 0), lo que sugiere que el dominio sobre objetivos del mapa depende de din√°micas distintas a las captadas por las otras variables.\n",
    "\n",
    "En conjunto, la matriz indica que el conjunto de variables **no est√° fuertemente correlacionado**, lo cual es favorable para el an√°lisis de cl√∫steres, ya que cada una aporta una dimensi√≥n distinta del rendimiento o comportamiento del jugador. Esto permitir√° que los m√©todos de agrupamiento (jer√°rquicos o no jer√°rquicos) detecten perfiles diferenciados basados en m√∫ltiples facetas del desempe√±o sin que ninguna variable domine excesivamente a las dem√°s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para obtener p-valores de cor.test\n",
    "p_matrix <- function(df){\n",
    "  vars <- colnames(df)\n",
    "  n <- length(vars)\n",
    "  M <- matrix(NA, n, n, dimnames = list(vars, vars))\n",
    "\n",
    "  for(i in 1:n){\n",
    "    for(j in 1:n){\n",
    "      M[i,j] <- cor.test(df[[i]], df[[j]])$p.value\n",
    "    }\n",
    "  }\n",
    "  return(M)\n",
    "}\n",
    "\n",
    "# Obtener matriz de p-valores\n",
    "p_values_table <- p_matrix(datos_est)\n",
    "\n",
    "# Mostrar la tabla\n",
    "p_values_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e653f",
   "metadata": {},
   "source": [
    "En un an√°lisis de correlaci√≥n, evaluamos si la relaci√≥n observada entre dos variables podr√≠a explicarse simplemente por azar. Para ello se plantea un contraste de hip√≥tesis:\n",
    "\n",
    "- **Hip√≥tesis nula ($H_0$)**:\n",
    "No existe correlaci√≥n real entre las dos variables.\n",
    "Matem√°ticamente:\n",
    "\n",
    "$$ùêª_0: ùúå=0$$\n",
    "\n",
    "- **Hip√≥tesis alternativa ($H_1$)**:\n",
    "S√≠ existe correlaci√≥n entre las dos variables (puede ser positiva o negativa).\n",
    "Matem√°ticamente:\n",
    "$$ùêª_1: ùúå‚â†0$$\n",
    "\n",
    "\n",
    "El p-valor indica la probabilidad de obtener una correlaci√≥n igual o m√°s extrema que la observada, asumiendo que la hip√≥tesis nula es cierta.\n",
    "\n",
    "Un **p-valor peque√±o (habitualmente p < 0.05**) sugiere que ser√≠a **muy improbable obtener esa correlaci√≥n por azar**, por lo que se **rechaza $H_0$** y se concluye que existe una correlaci√≥n significativa.\n",
    "\n",
    "Un **p-valor grande** implica que los datos observados son compatibles con ausencia de correlaci√≥n, por lo que **no se puede rechazar $H_0$**.\n",
    "\n",
    "En este caso todos son valores ampliamente superiores a 0.05 lo que indica que sus correlaciones son nulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caddba13",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5DADE2\"><b>Apartado 3</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91111f2",
   "metadata": {},
   "source": [
    "Ahora se calcular√° la matriz de proximidad o matriz de distancias entre las observaciones.\n",
    "\n",
    "Se usar√° la funci√≥n `dist()` para construir una matriz de distancias, que pertenece a la librer√≠a `base`. Se usar√° de la siguinete forma `dist(x, method = \"euclidean\")`, donde:\n",
    "\n",
    "+ **x**: es el data num√©rico que contiene las observaciones y las variables.\n",
    "\n",
    "+ **method**: indica el tipo de distancia que se desea calcular, en este caso se usar√° exclusivamnete:\n",
    "  - \"euclidean\" (distancia euclidiana, por defecto)  \n",
    "\n",
    "La funci√≥n devolver√° un objeto de clase `dist`, que almacena la matriz de distancias.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la matriz de distancias empleando la distancia euclidiana\n",
    "datos_dist = dist(datos_est, method = \"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb86e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la estructura de la matriz de distancias\n",
    "str(datos_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e891b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las primeras seis filas de la matriz de distancias\n",
    "matrix_dist = as.matrix(datos_dist)[1:6, 1:6]\n",
    "head(matrix_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos la librer√≠a factoextra\n",
    "install.packages(\"factoextra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c857c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la librer√≠a factoextra\n",
    "library(factoextra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos mediante escalas de color la distancia entre todas las observaciones para ver\n",
    "# si detectamos grupos de observaciones\n",
    "suppressWarnings(\n",
    "  fviz_dist(datos_dist, show_labels = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75f841",
   "metadata": {},
   "source": [
    "**<u>Interpretaci√≥n resultados</u>.-**   La distancia usada es la **distancia euclidiana** la cuantifica **cu√°n diferentes son dos observaciones** midiendo la longitud del segmento que las une en un espacio multidimensional.\n",
    "\n",
    "Si cada observaci√≥n viene dada por $p$ variables, la distancia entre dos individuos $x = (x_1,\\dots,x_p)$ e $y = (y_1,\\dots,y_p)$ se calcula como:\n",
    "\n",
    "$$\n",
    "d_{\\text{euclidiana}}(x,y)\n",
    "= \\sqrt{(x_1 - y_1)^2 + \\cdots + (x_p - y_p)^2 }.\n",
    "$$\n",
    "\n",
    "‚Äì **Cuanto menor es la distancia**, m√°s parecidas son las observaciones.  \n",
    "‚Äì **Cuanto mayor es la distancia**, m√°s diferentes son.  \n",
    "\n",
    "El mapa de distancias muestra la matriz de proximidad entre los 18 equipos tras tipificar las variables y calcular la **distancia euclidiana** entre cada par de ellos. Cada celda representa la distancia entre dos equipos:  \n",
    "‚Äì Tonos **rojizos** ‚Üí distancias bajas ‚Üí equipos **muy similares**.  \n",
    "‚Äì Tonos **azulados** ‚Üí distancias altas ‚Üí equipos **muy diferentes**.\n",
    "\n",
    "La diagonal principal aparece en rojo intenso, como es natural, ya que la distancia de cada equipo consigo mismo es cero.\n",
    "\n",
    "Al analizar el patr√≥n global, aparecen **zonas rojizas bien definidas** (especialmente hacia el centro del mapa, en la zona superior izquierda y la inferior derecha), lo que indica **bloques de equipos muy parecidos entre s√≠**, probablemente compartiendo niveles similares de win rate, horas de entrenamiento, control sobre los objetivos y ratios bajas/asistencias. Estos bloques sugieren la presencia de **cl√∫steres naturales y compactos**, que posteriormente deber√≠an emerger con claridad en algoritmos no jer√°rquicos.\n",
    "\n",
    "Por otro lado, se observan **manchas azul oscuro** localizadas, especialmente en la parte inferior derecha y tambi√©n en ala parte superior izquierda. Estas zonas se√±alan equipos que son **marcadamente distintos** al resto, ya sea por su desempe√±o u horas de entrenamiento. Es probable que estos equipos act√∫en como **observaciones aisladas** dentro del clustering o formen grupos muy reducidos.\n",
    "\n",
    "Finalmente, combinando las √°reas rojizas continuas con las zonas azuladas dispersas, el mapa sugiere la existencia de **entre dos y cuatro grupos** de equipos con perfiles diferenciados, lo cual respalda la pertinencia de avanzar hacia un an√°lisis de cl√∫ster no jer√°rquico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e644ab",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5DADE2\"><b>Apartado 4</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e4ef5",
   "metadata": {},
   "source": [
    "A diferencia de los m√©todos jer√°rquicos, que construyen una secuencia completa de particiones anidadas, los enfoques no jer√°rquicos generan directamente una √∫nica partici√≥n del conjunto, lo que exige especificar o estimar par√°metros clave como el n√∫mero de grupos, la densidad m√≠nima o la forma de los clusters. Esta caracter√≠stica los hace computacionalmente eficientes y particularmente adecuados para bases de datos de tama√±o medio o grande.\n",
    "\n",
    "En este estudio se considerar√°n **cuatro familias de m√©todos no jer√°rquicos**, cada una basada en un principio estructural diferente y capaz de capturar distintos tipos de patrones en los datos. Los **m√©todos basados en particiones** buscan la mejor asignaci√≥n de observaciones a k clusters optimizando una funci√≥n de distancia; los **m√©todos basados en densidad** detectan regiones densas y permiten descubrir grupos con formas arbitrarias; los **m√©todos basados en modelos** suponen que los datos provienen de una mezcla de distribuciones probabil√≠sticas y asignan probabilidades de pertenencia a cada cluster; y los **m√©todos basados en grafos** utilizan relaciones de similitud para identificar comunidades en estructuras complejas que no necesariamente siguen m√©tricas eucl√≠deas.\n",
    "\n",
    "Cada uno de estos enfoques ser√° estimado e interpretado de manera individual, analizando c√≥mo particionan los datos, qu√© tipo de estructura capturan y bajo qu√© supuestos operan. Este conjunto de m√©todos permite obtener una visi√≥n completa y comparada de la organizaci√≥n interna del conjunto de datos, destacando similitudes y diferencias entre estrategias basadas en geometr√≠a, densidad, probabilidad o conectividad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97ed2f",
   "metadata": {},
   "source": [
    "El algoritmo **k-means** es uno de los m√©todos de *clustering* m√°s utilizados debido a su eficiencia y simplicidad. Su objetivo es particionar un conjunto de datos en **k grupos no solapados**, de forma que cada observaci√≥n pertenezca al cluster cuyo centro (o **centroide**) est√© m√°s pr√≥ximo seg√∫n una m√©trica de distancia,en este caso la **distancia euclidiana**.\n",
    "El proceso es iterativo: primero asigna cada observaci√≥n al centroide m√°s cercano y despu√©s recalcula los centroides como la media de las observaciones asignadas. Este procedimiento se repite hasta que las asignaciones dejan de cambiar o se alcanza el n√∫mero m√°ximo de iteraciones. .\n",
    "\n",
    "Su soluci√≥n depende fuertemente de los valores iniciales de los centroides y puede converger a m√≠nimos locales. Por ello es habitual ejecutar varias inicializaciones aleatorias y seleccionar la mejor soluci√≥n.\n",
    "\n",
    "\n",
    "Para estimar un modelo k-means en R se utiliza la funci√≥n **`kmeans()`** de la librer√≠a *base*. Su sintaxis general es:\n",
    "\n",
    "`kmeans(x, centers, iter.max = 10, nstart = 1, algorithm = \"Hartigan-Wong\")`\n",
    "\n",
    "Los principales argumentos son:\n",
    "\n",
    "- **`x`**: *data.frame* con **variables num√©ricas** sobre las que se realizar√° el clustering en este caso ser√≠a datos_est.\n",
    "- **`centers`**: n√∫mero de clusters **k** o bien un conjunto de centroides iniciales.\n",
    "- **`nstart`**: n√∫mero de distintas **inicializaciones aleatorias** a probar para evitar soluciones en m√≠nimos locales ya comentada.\n",
    "- **`iter.max`**: n√∫mero m√°ximo de iteraciones permitidas del algoritmo.\n",
    "- **`algorithm`**: versi√≥n del algoritmo de optimizaci√≥n; por defecto `\"Hartigan-Wong\"` que es el m√°s habitual y eficiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos semilla para reproducibilidad\n",
    "set.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constru√≠mos el cl√∫ster no jer√°rquico con m√©todo k-means y distancia euclidiana\n",
    "NHC_kmeans = kmeans(datos_est, centers = 4, nstart = 25, iter.max = 100, algorithm = \"Hartigan-Wong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constatamos la clase del objeto\n",
    "class(NHC_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida del cl√∫ster no jer√°rquico con m√©todo k-means y distancia euclidiana\n",
    "str(NHC_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91942857",
   "metadata": {},
   "source": [
    "El objeto contiene $9$ elementos los cuales se explicar√°n a continuaci√≥n:\n",
    "\n",
    "El elemento `cluster` es un vector de longitud igual al n√∫mero de observaciones del conjunto de datos (18 en este caso). Cada posici√≥n contiene el **n√∫mero de cl√∫ster asignado** a la observaci√≥n correspondiente (el primer elemento estar√≠a en el cluster n√∫mero 4 junto al 7, 14 y 15). En k-means, esta asignaci√≥n es el resultado final del algoritmo tras iterar entre asignar puntos al centroide m√°s cercano y recalcular dichos centroides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c225c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero de cl√∫ster asignado a cada uno de los clientes\n",
    "NHC_kmeans$cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23aaf2",
   "metadata": {},
   "source": [
    "El componente `centers` es una matriz de dimensi√≥n **k √ó p** donde `k` es el n√∫mero de cl√∫steres y `p` el n√∫mero de variables empleadas. Cada fila contiene el **centroide** del cl√∫ster, expresado como la **media de cada variable dentro del grupo**.\n",
    "\n",
    "En k-means el centroide tiene un significado completamente geom√©trico: es el **promedio de las observaciones asignadas** al cl√∫ster en el espacio estandarizado. Esto contrasta con el clustering jer√°rquico, donde la noci√≥n de centroide depende del m√©todo de enlace. Analizar estos valores permite comprender el \"perfil medio\" de cada grupo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585bd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre los centroides de cada cl√∫ster\n",
    "NHC_kmeans$centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc0725",
   "metadata": {},
   "source": [
    "El elemento `totss` representa la **suma de cuadrados total** respecto a la media global del dataset. Es una medida de la **variabilidad total** de los datos antes de agrupar. Este valor no depende del n√∫mero de cl√∫steres `k` y sirve como referencia del total de variaci√≥n presente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94beae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre la suma de cuadrados total\n",
    "NHC_kmeans$totss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4956314",
   "metadata": {},
   "source": [
    "Este componente es un vector de longitud **k**, donde cada elemento mide la **suma de cuadrados intra‚Äìcl√∫ster** del grupo correspondiente. Representa la **compactaci√≥n interna** del cl√∫ster: valores bajos indican que las observaciones del grupo est√°n muy pr√≥ximas entre s√≠, mientras que valores altos reflejan alta dispersi√≥n.\n",
    "\n",
    "En este caso el clusters 4 ser√≠a el m√°s compacto y el 3 ser√≠a el que posee m√°s dispersi√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre la suma de cuadrados intra-cl√∫ster de cada cl√∫ster\n",
    "NHC_kmeans$withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de608083",
   "metadata": {},
   "source": [
    "`tot.withinss` es la suma total de todas las `withinss`. Cuantifica la **variabilidad que queda dentro de los cl√∫steres** tras realizar la partici√≥n. Es precisamente el valor que se usa para construir el **m√©todo del codo**, comparando c√≥mo disminuye esta magnitud cuando se aumenta el n√∫mero de cl√∫steres.\n",
    "\n",
    "Una buena elecci√≥n de `k` se refleja en valores relativamente bajos de `tot.withinss`, pero sin caer en aumentos exagerados de complejidad (a√±adir m√°s cl√∫steres de los necesarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre el total de la suma de cuadrados intra-cl√∫ster\n",
    "NHC_kmeans$tot.withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511c882",
   "metadata": {},
   "source": [
    "El valor `betweenss` representa la **variabilidad explicada por la separaci√≥n entre los cl√∫steres**, es decir, por la distancia entre sus centroides. Te√≥ricamente cumple que:\n",
    "\n",
    "$$\\text{totss} \\approx \\text{tot.withinss} + \\text{betweenss}.$$\n",
    "\n",
    "Cuanto mayor sea `betweenss` en proporci√≥n al total, mejor **separados** est√°n los grupos y m√°s estructura est√° captando el modelo. Por tanto, un clustering de buena calidad presenta `betweenss` alto y `tot.withinss` bajo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71642d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre la suma de cuadrados entre c√∫steres\n",
    "NHC_kmeans$betweenss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f08d70",
   "metadata": {},
   "source": [
    "El componente `size` es un vector de longitud **k** que indica el **n√∫mero de observaciones en cada cl√∫ster**. Revisar estos tama√±os es crucial, ya que cl√∫steres extremadamente peque√±os pueden sugerir:\n",
    "\n",
    "- presencia de **outliers** agrupados,  \n",
    "- un cl√∫ster poco estable,  \n",
    "- o una mala elecci√≥n de `k`.\n",
    "\n",
    "Cl√∫steres m√°s equilibrados suelen indicar una partici√≥n m√°s robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre el no. de observaciones en cada cl√∫ster\n",
    "NHC_kmeans$size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac497f27",
   "metadata": {},
   "source": [
    "El valor `iter` indica cu√°ntas **iteraciones** necesit√≥ el algoritmo k-means para converger. El algoritmo se detiene cuando las asignaciones no cambian o cuando se alcanza el m√°ximo de iteraciones permitido.\n",
    "\n",
    "Un n√∫mero reducido de iteraciones suele implicar que los grupos han sido identificados con claridad desde el inicio. Por el contrario, muchas iteraciones pueden sugerir geometr√≠a m√°s compleja o que los centroides iniciales estaban mal posicionados.\n",
    "\n",
    "En este caso 2 ieraciones es un valor muy bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre el no. de iteraciones del algoritmo\n",
    "NHC_kmeans$iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90d57b",
   "metadata": {},
   "source": [
    "Finalmente, `ifault` es un **c√≥digo de estado** que informa sobre posibles problemas durante la ejecuci√≥n. El valor `0` implica que **la convergencia ha sido correcta**, sin errores num√©ricos como en este caso. Cualquier otro valor indicar√≠a problemas como: falta de convergencia, centroides vac√≠os o errores en la inicializaci√≥n.\n",
    "\n",
    "Este componente permite garantizar que la soluci√≥n obtenida es fiable antes de interpretarla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre la convergencia del algoritmo\n",
    "NHC_kmeans$ifault"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990e7a1",
   "metadata": {},
   "source": [
    "El m√©todo **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** es uno de los algoritmos m√°s utilizados para detectar **grupos de formas arbitrarias** y **manejar autom√°ticamente el ruido**. A diferencia de K-means, no requiere fijar el n√∫mero de cl√∫steres y basa su funcionamiento en la idea de **densidad local** alrededor de cada punto. Esto lo convierte en una herramienta muy √∫til cuando los grupos no son esf√©ricos o cuando existen observaciones at√≠picas.\n",
    "\n",
    "El algoritmo parte de dos par√°metros clave:  \n",
    "- un **radio de vecindad** llamado $\\epsilon$ (`eps`),  \n",
    "- un **m√≠nimo de puntos** dentro de ese radio denominado `minPts`.\n",
    "\n",
    "A partir de estos par√°metros, DBSCAN clasifica cada observaci√≥n seg√∫n la densidad de su entorno. El procedimiento general consta de los siguientes pasos:\n",
    "\n",
    "1. **Para cada punto**, se calcula cu√°ntos vecinos tiene dentro del radio $\\epsilon$.  \n",
    "2. **Si el n√∫mero de vecinos ‚â• minPts**, el punto se considera **n√∫cleo** y se inicia (o expande) un cl√∫ster.  \n",
    "3. A partir de cada punto n√∫cleo, el algoritmo **\"crece\" el cl√∫ster** incorporando:\n",
    "   - todos sus vecinos directos,\n",
    "   - los vecinos de esos vecinos, siempre que sean tambi√©n puntos n√∫cleo.\n",
    "4. Los puntos que no cumplen las condiciones de densidad se examinan:\n",
    "   - si est√°n cerca de un punto n√∫cleo, se consideran **frontera**,  \n",
    "   - si no pertenecen a ninguna regi√≥n densa, se clasifican como **ruido**.\n",
    "5. Se repite el proceso hasta revisar todos los puntos.\n",
    "\n",
    "El resultado es una partici√≥n formada por **regiones densas** separadas por **zonas de baja densidad**, con la ventaja de que el m√©todo **no fuerza asignaciones**: permite dejar puntos sin cl√∫ster .\n",
    "\n",
    "\n",
    "\n",
    "Como ya se ha mencionado el algoritmo distingue tres categor√≠as de puntos:\n",
    "\n",
    "**a) Puntos n√∫cleo (core points)**\n",
    "Son los puntos cuya **vecindad de radio $\\epsilon$ contiene al menos `minPts`** observaciones (incluido el propio).  \n",
    "Representan las regiones donde la densidad es suficientemente alta como para formar un cl√∫ster.\n",
    "\n",
    "En t√©rminos geom√©tricos:  \n",
    "$$|N_\\epsilon(x)| \\geq \\text{minPts}.$$\n",
    "\n",
    "Estos puntos **generan y expanden los cl√∫steres**.\n",
    "\n",
    "**b) Puntos frontera (border points)**\n",
    "Son puntos que **no cumplen** el criterio de densidad para ser n√∫cleo, pero **est√°n dentro del radio $\\epsilon$ de un punto n√∫cleo**.  \n",
    "No generan nuevos cl√∫steres, pero s√≠ **se conectan a uno ya existente**.\n",
    "\n",
    "Representan observaciones situadas en la periferia de los grupos densos.\n",
    "\n",
    "**c) Puntos ruido (noise points u outliers)**\n",
    "Son observaciones que **no cumplen ninguna de las dos condiciones anteriores**.  \n",
    "No est√°n suficientemente cerca de ning√∫n punto n√∫cleo y, por tanto, **no pertenecen a ning√∫n cl√∫ster**.\n",
    "\n",
    "Estos puntos son especialmente relevantes, ya que DBSCAN identifica de forma natural los **outliers estructurales** del dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c56337",
   "metadata": {},
   "source": [
    "La funci√≥n principal del paquete **`dbscan`** para implementar este m√©todo es: **`dbscan(x, eps, minPts, weights = NULL, borderPoints = TRUE, search = \"kd\", bucketSize = 10, splitRule = \"suggest\", ...)`** donde:\n",
    "\n",
    "\n",
    "\n",
    "+ `x`: Matriz o data frame con las **variables num√©ricas estandarizadas** del clustering al igual que en kmeans no se introduce la matriz de distancias.\n",
    "\n",
    "+ `eps`: Distancia m√°xima que define la **vecindad** de un punto.  \n",
    "  Es el par√°metro m√°s importante: define qu√© tan \"juntos\" deben estar los puntos para considerarse parte de una misma regi√≥n densa.\n",
    "\n",
    "\n",
    "+ `minPts`:**N√∫mero m√≠nimo de puntos** contandose a si mismo que debe haber en un entorno de radio `eps` para considerar que un punto es un **punto n√∫cleo (core point)**. Con este par√°metro se modera la densidad para formar un cl√∫ster.\n",
    "\n",
    "+ `weights`: Vector de pesos para ponderar la densidad local. Si no se especifica, todos los puntos tienen peso $1$ como ser√° en este caso.\n",
    "\n",
    "+ `borderPoints`: Indica si los puntos frontera deben asignarse a un cl√∫ster (`TRUE`) o mantenerse como ruido (`FALSE`).\n",
    "\n",
    "\n",
    "+ `search`: Tipo de estructura usada para b√∫squedas de vecinos. En este caso se usar√°  `\"kd\"` (la cual est√° por defecto) que utiliza un kd-tree, muy eficiente en dimensiones bajas/medias.\n",
    "\n",
    "+ `bucketSize`: Controla el tama√±o m√≠nimo de los nodos del kd-tree, afectando al rendimiento. El valor por defecto es $10$, adecuado para este caso.\n",
    "\n",
    "+ `splitRule`: Regla de divisi√≥n del kd-tree. No cambia el resultado, solo la eficiencia computacional por lo que se dejar√° por defecto.\n",
    "\n",
    "En la funci√≥n hay par√°metros avanzados opcionales para b√∫squedas o matrices de distancias de ah√≠los puntos suspensivos en la sintaxis, al no utilizarse no se explicar√°n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee99d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de la librer√≠a dbscan\n",
    "install.packages(\"dbscan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la librer√≠a dbscan\n",
    "library(dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constru√≠mos el cl√∫ster no jer√°rquico con m√©todo dbscan\n",
    "# eps: radio de vecindad\n",
    "# minPts: m√≠nimo n√∫mero de puntos para formar una regi√≥n densa\n",
    "NHC_dbscan = dbscan(datos_est, eps = 1.5, minPts = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constatamos la clase del objeto\n",
    "class(NHC_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ee64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida del cl√∫ster no jer√°rquico con bdscan\n",
    "str(NHC_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf391a96",
   "metadata": {},
   "source": [
    "Los valores utilizados para el modelo se han variado debido a que si se redujer√° la distancia o se aumentar√° el n√∫mero de nodos necesario los clusters se reducen a un √∫nico cluster o identifica a todos como outliers.\n",
    "\n",
    "El objeto contiene $5$ elementos los cuales se explicar√°n a continuaci√≥n:\n",
    "\n",
    "El elemento `cluster` es un vector de longitud igual al n√∫mero de observaciones del conjunto de datos (18 en este caso). Cada posici√≥n contiene el **n√∫mero de cl√∫ster asignado** a la observaci√≥n correspondiente (el primer elemento estar√≠a en el cluster n√∫mero 1 junto al 2, 7, 11, 14 y 15). Los elementos que aparecen con un 0 significa que se han identificado como un outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc007e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero de cl√∫ster asignado a cada uno de los clientes\n",
    "NHC_dbscan$cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff4f76",
   "metadata": {},
   "source": [
    "El objeto **`eps`** indica el radio de vecindad $\\epsilon$ usado por DBSCAN. Representa la **distancia m√°xima para que dos puntos se consideren vecinos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61084b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre el radio de vencidad utilizado\n",
    "NHC_dbscan$eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f001e99",
   "metadata": {},
   "source": [
    "El objeto **`minPts`** indica el **n√∫mero m√≠nimo de puntos necesarios** usado por DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cc6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre el no. m√≠nimo de puntos necesarios par considerar un cl√∫ster\n",
    "NHC_dbscan$minPts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823a55",
   "metadata": {},
   "source": [
    "Este objeto  **`metric`** indica la **m√©trica empleada para calcular distancias** entre observaciones usada por DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre la m√©trica utilizada\n",
    "NHC_dbscan$metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde973d",
   "metadata": {},
   "source": [
    "Este objeto **`borderPoints`** indica si los puntos frontera se incluyen en los cl√∫steres. En este caso se usao el por defecto `TRUE` lo que significa es que los puntos frontera se asignan al cl√∫ster correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre si se consideran los puntos frontera o no\n",
    "NHC_dbscan$borderPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434698c9",
   "metadata": {},
   "source": [
    "El m√©todo **Gaussian Mixture Models (GMM)** es un enfoque probabil√≠stico para el clustering donde se asume que los datos proceden de una **mezcla de distribuciones gaussianas multivariantes**, cada una representando un cl√∫ster. A diferencia de k-means (que asigna cada punto a un √∫nico cl√∫ster), GMM asigna **probabilidades de pertenencia**, permitiendo capturar cl√∫steres con **formas elipsoidales**, **distintas varianzas** y **correlaciones internas** entre variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8e4b1",
   "metadata": {},
   "source": [
    "El procedimiento general consta de los siguinetes pasos:\n",
    "1. Se generan estimaciones iniciales de medias (vector centroide), matrices de covarianza y pesos de mezcla. Si no se inicializan se har√° una asignaci√≥n aleatoria.\n",
    "\n",
    "2. Antes de asignar una observaci√≥n a un cluster a cada observaci√≥n el algoritmo calcula la **probabilidad posterior de pertenecer a cada cluster**, es decir, las **responsabilidades**:  \n",
    "  $$\\tau_{ik} = P(\\text{corresponder al cl√∫ster } k \\mid x_i).$$\n",
    "   Esto permite modelar incertidumbre: un equipo puede tener un $70 \\%$ de pertenencia al cl√∫ster 1 y un $30 \\%$ al cl√∫ster 3 o tener un $52 \\%$ y un $48 \\%$ en cuyo caso no estr√≠a tan claro a cual pretenece.\n",
    "  \n",
    "3. A partir de las responsabilidades, se actualizan los par√°metros del modelo: medias, matrices de covarianza y pesos. Y con cada iteraci√≥n se mejora la verosimilitud del modelo.\n",
    "\n",
    "4. Los pasos anteriores se repiten hasta que la log-verosimilitud se estabiliza o se alcanza el n√∫mero m√°ximo de iteraciones. Esta naturaleza iterativa hace que GMM sea m√°s flexible que m√©todos como DBSCAN y m√°s expresivo que k-means.\n",
    "\n",
    "5. Tras relizar todos lo pasos previos la observaci√≥n se asocia al cluster con mayor probabilidad.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99efb15",
   "metadata": {},
   "source": [
    "La funci√≥n principal para estimar un GMM en R es: **`Mclust(data, G = NULL, modelNames = NULL, prior = NULL, control = emControl(), initialization = NULL, ...)`**. A continuaci√≥n se explican sus argumentos m√°s importantes:\n",
    "\n",
    "- **`data`**: matriz o data.frame con las variables num√©ricas. Al igual que los m√©todos anteriores no utiliza matriz de distancias.\n",
    "\n",
    "- **`G`**: conjunto de n√∫meros de componentes (clusters) a evaluar. Si se deja por defecto, se usa autom√°ticamente `1:9` lo que significa que se har√° con $G=1$ hasta $G=9$. El modelo √≥ptimo se selecciona por BIC.\n",
    "\n",
    "- **`modelNames`**: define la estructura de las matrices de covarianza (en forma: esf√©ricas, diagonales, elipsoidales o en volumen). Si se deja `NULL`, se prueban modelos adecuados seg√∫n la dimensi√≥n del dataset y tambi√©n se elige el mejor seg√∫n BIC.\n",
    "\n",
    "- **`prior`**: especifica un prior conjugado para medias y covarianzas mediante `priorControl()`. Se usar√° su valor por defecto `NULL` (estimaci√≥n por m√°xima verosimilitud sin informaci√≥n previa).\n",
    "\n",
    "- **`control`**: par√°metros del algoritmo EM (tolerancias, iteraciones m√°ximas), definidos con `emControl()`. Permitiendo un mayor ajuste sobre el modelo.\n",
    "\n",
    "\n",
    "\n",
    "- **`initialization`**: controla la inicializaci√≥n del EM. Puede incluir:\n",
    "  - `hcPairs` (partici√≥n jer√°rquica previa),\n",
    "  - `subset` (subconjunto para inicializar),\n",
    "  - `noise` (componente adicional para ruido).\n",
    "  Si no se especifica, `Mclust()` realiza autom√°ticamente una inicializaci√≥n jer√°rquica robusta.\n",
    "\n",
    "En la funci√≥n hay par√°metros avanzados opcionales para la representaci√≥n de gr√°ficas de ah√≠ los puntos suspensivos en la sintaxis, al no utilizarse no se explicar√°n.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa069b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de la librer√≠a mclust\n",
    "install.packages(\"mclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47905c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la librer√≠a mclust\n",
    "library(mclust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constru√≠mos el cl√∫ster no jer√°rquico con modelos de mezclas gaussianas\n",
    "# data: base de datos estandarizada (datos_est)\n",
    "# G = 2:4 orquilla de 2 a 4 componentes (cl√∫steres) para facilitar la comparaci√≥n con k-means y DBSCAN\n",
    "NHC_gmm = Mclust(data = datos_est, G = 2:4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a665000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constatamos la clase del objeto\n",
    "class(NHC_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4793dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida del cl√∫ster no jer√°rquico con modelos de mezclas gaussianas\n",
    "str(NHC_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e650801",
   "metadata": {},
   "source": [
    "El objeto contiene 16  elementos los cuales se explicar√°n a continuaci√≥n:\n",
    "\n",
    "+ **`cluster`**: Muestra la **instrucci√≥n exacta utilizada para estimar el modelo**. Igual que en clustering jer√°rquico (`call` en `hclust`) o en k-means, sirve como referencia documental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrucci√≥n exacta utilizada para estimar el modelo GMM\n",
    "NHC_gmm$call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178253dd",
   "metadata": {},
   "source": [
    "+ **`data`**: Contiene **los datos estandarizados utilizados en el modelo**.  \n",
    "Es una matriz num√©rica de dimensi√≥n **18√ó4** (18 observaciones, 4 variables).  \n",
    "Incluye nombres de las variables:  \n",
    "`Win_rate`, `KDA`, `Control_obj`, `Entreno_horas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de datos utilizada (660 observaciones x 5 variables estandarizadas)\n",
    "head(NHC_gmm$data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f226233",
   "metadata": {},
   "source": [
    "+ **`modelName`**: Indica el **modelo de la matriz de covarianzas utilizado**. En este caso `\"VII\"` es el **tipo de estructura de covarianza elegido por BIC** entre todas las posibles.  \n",
    "En este caso:  \n",
    "- V = volumen variable  \n",
    "- I = forma esf√©rica  \n",
    "- I = orientaci√≥n no aplicable (esf√©rico)\n",
    "\n",
    "Modelo **esf√©rico con varianzas diferentes entre grupos** el resto de posisbilidades est√°n detallasdas m√°s abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a65e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo de modelo gaussiano elegido (VVV = covarianzas completamente libres)\n",
    "NHC_gmm$modelName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e55ae3",
   "metadata": {},
   "source": [
    "+ **`n`**: Indica el **n√∫mero de observaciones** consideradas en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero total de observaciones\n",
    "NHC_gmm$n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd5d19",
   "metadata": {},
   "source": [
    "+ **`d`**: Indica el **n√∫mero de variables** consideradas en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero de variables (dimensi√≥n del espacio)\n",
    "NHC_gmm$d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518183b9",
   "metadata": {},
   "source": [
    "+ **`G`**: Indica el **n√∫mero de componentes (cl√∫steres)** seleccionados por el algoritmo EM (Expectation Maximization) entre los propuestos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero de componentes gaussianos seleccionados\n",
    "NHC_gmm$G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75979824",
   "metadata": {},
   "source": [
    "+ **`BIC`**: contiene **los valores del BIC para todos los modelos probados**.  \n",
    "Es una matriz con:\n",
    "- filas: componentes evaluadas (`2`, `3`, `4`)\n",
    "- columnas: tipos de modelos gaussianos (`EII`, `VII`, `EEI`, ‚Ä¶)\n",
    "\n",
    "Se usa para seleccionar el mejor modelo.\n",
    "\n",
    "En la celda de c√≥digo se explican todas las posibles combinaciones de los modelos gaussianos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de valores BIC para los diferentes modelos gaussianos evaluados\n",
    "# 1¬∫ letra (Volumen): Tama√±o del cl√∫ster\n",
    "# 2¬∫ letra (Forma): Relaci√≥n entre las longitudes de los ejes principales - elipticidad\n",
    "# 3¬∫ letra (Orientaci√≥n): Rotaci√≥n de la elipse en el espacio - matriz de eigenvectores\n",
    "\n",
    "# EII: Volumen igual, forma esf√©rica, sin orientaci√≥n -> Cl√∫steres esf√©ricos, del mismo tama√±o, sin rotaci√≥n. Es el modelo equivalente a k-means\n",
    "# VII: Volumen distinto, forma esf√©rica, sin orientaci√≥n -> Cl√∫steres esf√©ricos pero con tama√±os diferentes.\n",
    "# EEI: Volumen igual, forma igual, sin orientaci√≥n -> Elipses con misma forma pero no rotadas; s√≥lo difieren en la ubicaci√≥n de su centro.\n",
    "# VEI: Volumen distinto, misma forma, sin orientaci√≥n -> Mismas elipses sin rotaci√≥n, pero unas m√°s grandes que otras.\n",
    "# EVI: Volumen igual, forma distinta, sin orientaci√≥n -> Todas las elipses tienen mismo volumen pero diferente elipticidad.\n",
    "# VVI: Volumen distinto, forma distinta, sin orientaci√≥n -> Elipses sin rotaci√≥n pero cada cl√∫ster tiene volumen y forma distinta.\n",
    "# EEE: Volumen igual, forma igual, orientaci√≥n igual -> Misma elipse (misma covarianza) en todos los cl√∫steres.\n",
    "# VEE: Volumen distinto, forma igual, orientaci√≥n igual -> Cl√∫steres con estructura id√©ntica pero tama√±os distintos.\n",
    "# EVE: Volumen igual, forma distinta, orientaci√≥n igual -> Diferente elipticidad pero misma rotaci√≥n.\n",
    "# VVE: Volumen distinto, forma distinta, orientaci√≥n igual -> Diferencias en tama√±o y forma, pero mismos ejes de orientaci√≥n.\n",
    "# EEV: Volumen igual, forma igual, orientaci√≥n distinta -> Todas las elipses tienen tama√±o y forma igual pero est√°n rotadas de distinta forma.\n",
    "# VEV: Volumen distinto, forma igual, orientaci√≥n distinta -> Vol√∫menes diferentes, misma forma, orientaciones distintas.\n",
    "# EVV: Volumen igual, forma distinta, orientaci√≥n distinta -> Mismo volumen pero formas y orientaciones diferentes.\n",
    "# VVV: Volumen distinto, forma distinta, orientaci√≥n distinta -> El modelo m√°s flexible de todos, sin restricciones.\n",
    "NHC_gmm$BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b9c51",
   "metadata": {},
   "source": [
    "+ **`loglik`**: Valor de la **log-verosimilitud final** del modelo. Cuanto mayor sea, mejor ajusta el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70616b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-verosimilitud del modelo GMM estimado\n",
    "NHC_gmm$loglik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464dc69",
   "metadata": {},
   "source": [
    "+ **`df`**: Grados de libertad del modelo (**n√∫mero de par√°metros estimados**). Dependen de las medias, pesos y covarianzas a estimar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6453ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grados de libertad (par√°metros estimados del modelo)\n",
    "NHC_gmm$df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9213cf6",
   "metadata": {},
   "source": [
    "+ **`bic`**: Valor del BIC final del modelo elegido (el mayor entre todas las combinaciones probadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor del BIC del modelo finalmente elegido por Mclust\n",
    "NHC_gmm$bic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8637a",
   "metadata": {},
   "source": [
    "+ **`icl`**: Valor del ICL (Integrated Completed Likelihood), penaliza el solapamiento entre clusters.  \n",
    "Valor cercano al BIC indica que los cl√∫steres est√°n bien separados. Como se puede apreciar la diferencia es inferior a una unidad inferior al $1 \\%$ del Bic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor del √≠ndice ICL (penaliza incertidumbre de clasificaci√≥n)\n",
    "NHC_gmm$icl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9943bf",
   "metadata": {},
   "source": [
    "+ **`hypvol`**: Volumen del hiperparalelogramo para modelos 1D.  \n",
    "En este caso es `NA` porque el modelo es multivariante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volumen hiperdimensional del espacio de par√°metros (habitualmente NA)\n",
    "NHC_gmm$hypvol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c60fed",
   "metadata": {},
   "source": [
    "+ **`parameters`**:Lista con los par√°metros del modelo GMM seleccionado.\n",
    "\n",
    "a) `pro`\n",
    "Pesos de mezcla: $0.85\\%$ y $0,15\\%$ lo que siginifica que el **cl√∫ster 1 contiene el 85.3%** de los jugadores; el **cl√∫ster 2 el 14.7%**.\n",
    "\n",
    "b)`mean`\n",
    "Matriz de medias (**4 variables √ó 2 componentes**), una columna por cl√∫ster.  \n",
    "En este caso:\n",
    "- Cl√∫ster 1: jugadores con Win_rate positivo.  \n",
    "- Cl√∫ster 2: jugadores con m√©tricas notablemente menores.\n",
    "\n",
    "c) `variance`\n",
    "Estructura de covarianza donde se pueden extraer m√∫ltiples datos, pero solo se extraer√°:\n",
    "\n",
    "- `sigma`: matrices de covarianza completas (4√ó4√ó2).  \n",
    "  Cada componente produce una matriz distinta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporciones de mezcla (probabilidad a priori de cada cl√∫ster)\n",
    "NHC_gmm$parameters$pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ef771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medias (centroides gaussianos) por cl√∫ster y variable\n",
    "NHC_gmm$parameters$mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de covarianza para cada componente (definen forma y orientaci√≥n)\n",
    "str(NHC_gmm$parameters$variance$sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la matriz de covarianza del cl√∫ster 1\n",
    "cov_1 = NHC_gmm$parameters$variance$sigma[,,1]\n",
    "cov_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la matriz de covarianza del cl√∫ster 2\n",
    "cov_2 = NHC_gmm$parameters$variance$sigma[,,2]\n",
    "cov_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f90a0c",
   "metadata": {},
   "source": [
    "+ **`z`**: Matriz de probabilidades de pertenencia (**responsabilidades EM**).  \n",
    "Con una dimensi√≥n: **18√ó2** ya que son 18 equipos y solo dos clusters. Los valores oscilan entre $0$ y $1$. En este caso la primera observaci√≥n $1.00$, $0.00$ indica que el equipo es pr√°cticamente seguro que se encuentra en el cl√∫ster $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidades de pertenencia de cada observaci√≥n a cada cl√∫ster\n",
    "NHC_gmm$z[1:5, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d78ac",
   "metadata": {},
   "source": [
    "+ **`classification`**: Vector con la **asignaci√≥n final** de cada observaci√≥n siguinedo la regla del m√°ximo a posteriori. En este caso el quipo n√∫mero 4, 6 y 12 pertenecer√≠an al segundo cluster y el resto pertenecer√≠an al primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5483c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificaci√≥n final de cada cliente (cl√∫ster con mayor probabilidad)\n",
    "NHC_gmm$classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15cf10",
   "metadata": {},
   "source": [
    "+ **`uncertainty`**: Mide la **incertidumbre de clasificaci√≥n** siguiendo la siguinete ecuaci√≥n:  \n",
    "$$1 - \\max_k(\\tau_{ik})$$\n",
    "Valores cercanos a **0** indican que la clasificaci√≥n muy segura.  \n",
    "Mientras que los valores altos indican la existencia de puntos en frontera entre cl√∫steres.\n",
    "\n",
    "En este caso como se puede observar ser√≠an muy proximos a 0 lo que refuerza la divisi√≥n realizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incertidumbre asociada a la clasificaci√≥n de cada observaci√≥n\n",
    "head(NHC_gmm$uncertainty, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638d61f",
   "metadata": {},
   "source": [
    "El m√©todo **Spectral Clustering (SPEC)** es un enfoque basado en grafos que realiza el clustering analizando la **estructura de conectividad** entre las observaciones, en lugar de asumir distribuciones estad√≠sticas como en GMM. El algoritmo construye una matriz de afinidad que mide la similitud entre pares de puntos y, a partir de ella, obtiene **la matriz Laplaciana**. Estos autovectores transforman los datos a un espacio vectorial donde los cl√∫steres se vuelven linealmente separables, permitiendo aplicar k-means de forma eficaz. A diferencia de m√©todos basados en centroides o distribuci√≥n, SPEC puede detectar cl√∫steres no convexos, estructuras complejas y relaciones que dependen de la geometr√≠a del grafo, sin suponer gaussianidad ni formas elipsoidales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523ccfa",
   "metadata": {},
   "source": [
    "El procedimiento general consta de los siguinetes pasos:\n",
    "\n",
    "- **1. Construcci√≥n del modelo a partir de un grafo**\n",
    "Spectral Clustering comienza interpretando los datos como un **grafo de similitud**:\n",
    "  - Cada observaci√≥n es un **nodo**.\n",
    "  - La similitud entre observaciones define una **arista** del grafo.\n",
    "  - El usuario debe fijar el n√∫mero de cl√∫steres $K$.\n",
    "\n",
    "- **2. Construcci√≥n de la matriz de afinidad, grado y Laplaciana**\n",
    "El algoritmo construye internamente:\n",
    "\n",
    "  - **Matriz de afinidad** $W$: usando un kernel (t√≠picamente RBF o KNN), calcula cu√°nta similitud hay entre pares de puntos.\n",
    "  - **Matriz de grado** $D$: contiene la suma de las afinidades de cada nodo.\n",
    "  - **Matriz laplaciana** $L = D - W$: describe la estructura del grafo y ser√° la base del an√°lisis espectral.\n",
    "- **3. Descomposici√≥n espectral**\n",
    "Se realiza una descomposici√≥n en autovalores y autovectores:\n",
    "\n",
    "  - Los **autovalores m√°s peque√±os** contienen la estructura natural del grafo.\n",
    "  - Se descarta el **primer autovector** $\\mathbf{v}_1$ ya que est√° asociado al autovalor 0.\n",
    "  - Se eligen los **$K$ autovectores siguientes**: $\\mathbf{v}_2, \\dots, \\mathbf{v}_{K+1}$.\n",
    "  - Estos autovectores forman la **matriz espectral** $U$, que reubica los datos en un espacio donde los cl√∫steres son linealmente separables.\n",
    "\n",
    "- **4. Clustering en el espacio espectral**\n",
    "  En este nuevo espacio (filas de $U$), ya no se trabaja con las coordenadas originales:\n",
    "  - Se aplica **k-means** sobre las filas de $U$.\n",
    "  - k-means funciona correctamente aqu√≠ porque los cl√∫steres ya est√°n separados de forma casi lineal.\n",
    "\n",
    "- **5. Asignaci√≥n final de cl√∫steres**\n",
    "  El algoritmo asigna directamente sin hacer iteracions una etiqueta por punto:\n",
    "  - No existen puntos ‚Äúborder‚Äù o ‚Äúcore‚Äù (DBSCAN).\n",
    "  - No existen probabilidades ni incertidumbres (GMM).\n",
    "  - No hay ruido expl√≠cito.\n",
    "  - Los cl√∫steres pueden tener **formas no convexas**, algo que GMM y k-means no pueden capturar en el espacio original.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b28553",
   "metadata": {},
   "source": [
    "La funci√≥n principal es `specc()`,de la librer√≠a $\\tt kernlab$. Cuya sintaxis es **`specc(x, centers, kernel = \"rbfdot\", kpar = \"automatic\", iterations = 200, na.action = na.omit, ...)`** donde:\n",
    "\n",
    "+ `x`: Matriz o data.frame con datos num√©ricos estandarizados. A partir de ellos se construye autom√°ticamente la matriz de afinidad y el grafo de similitud.\n",
    "\n",
    "+ `centers`: N√∫mero de cl√∫steres $K$ o matriz de centros iniciales en el espacio espectral. En este caso, se especificar√°  `centers = 2`.\n",
    "\n",
    "+ `kernel`: especifica el tipo de kernel usado para calcular la afinidad.En esta pr√°ctica se usar√° kernel = \"rbfdot\" ‚Üí kernel Gaussiano (RBF). Aunque habr√≠a otras obciones como \"laplacedot\", \"polydot\", \"vanilladot\" entre otros.\n",
    "\n",
    "+ `kpar`: controla los **par√°metros del kernel**. Puede ser una cadena de texto, como `\"automatic\"`, que elige valores adecuados autom√°ticamente, o una lista, pkpar = list(sigma = 0.05). Aunque en este caso se usar√° el por defecto.\n",
    "\n",
    "+ `iterations`: n√∫mero **m√°ximo de iteraciones internas** del algoritmo k-means. Por defecto suele ser `iterations = 200`.\n",
    "\n",
    "+ `na.action`: indica **c√≥mo tratar los valores perdidos** (`NA`). Por defecto se eliminan utilizando `na.action = na.omit`.\n",
    "\n",
    "En la funci√≥n hay par√°metros avanzados opcionales para la normalizaci√≥n del grafo, ajustes de k-means u opciones del kernel de ah√≠ los puntos suspensivos en la sintaxis, al no utilizarse no se explicar√°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48cf7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de la librer√≠a kernlab\n",
    "install.packages(\"kernlab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la librer√≠a kernlab\n",
    "library(kernlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb339999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ayuda de la funci√≥n specc(x, centers, kernel, kpar, ...)\n",
    "?specc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0827847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constru√≠mos el cl√∫ster no jer√°rquico con Spectral Clustering\n",
    "# x: base de datos estandarizada (datos_est)\n",
    "# centers : fijamos 2 cl√∫steres para facilitar la comparaci√≥n con k-means, DBSCAN y GMM\n",
    "NHC_spec = specc(x = as.matrix(datos_est), centers = 2, kernel  = \"rbfdot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b891e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constatamos la clase del objeto\n",
    "class(NHC_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e486665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida del cl√∫ster no jer√°rquico con Spectral Clustering\n",
    "str(NHC_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe45b9",
   "metadata": {},
   "source": [
    "El objeto contiene 5 elementos los cuales se explicar√°n a continuaci√≥n:\n",
    "\n",
    "+ **`Data`**: Es un vector de enteros que contiene la asignaci√≥n final de cl√∫steres para cada observaci√≥n.\n",
    "\n",
    "En este caso habr√≠a 9 equipos en cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector con la asignaci√≥n final de cl√∫ster para cada observaci√≥n (1 a 660)\n",
    "NHC_spec@.Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b121c7",
   "metadata": {},
   "source": [
    "+ **`centers`**: Indica la informaci√≥n sobre los **centroides en el espacio transformado**. Aunque el clustering se hace en el espacio espectral (autovectores del Laplaciano del grafo), `specc()` devuelve los **centroides reconstruidos en el espacio original** para facilitar la interpretaci√≥n. No son los centroides reales del clustering, sino una aproximaci√≥n √∫til para interpretar perfiles medios de cada cl√∫ster.\n",
    "\n",
    "Hay dos filas ya que solo se ha dividido en dos clusters, en este caso da la casualidad de que est√°n exactamente en el extremo opuesto $C_1 = -C_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e49db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de 2 x 5 con los centroides de los cl√∫steres en el espacio original\n",
    "NHC_spec@centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a4d25",
   "metadata": {},
   "source": [
    "+ **`size`**: Es un vector que indica el n√∫mero de puntos que pertenecen a cada cl√∫ster. Como ya se ha comentado estna en euilibrio 9 en cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero de observaciones asignadas a cada cl√∫ster\n",
    "NHC_spec@size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612c70e",
   "metadata": {},
   "source": [
    "+ **`kernelf`**: Es un objeto que define el kernel utilizado para construir la matriz de afinidad del grafo en este caso representa al kernel gaussiano $K(x, y) = \\exp\\!\\left(-\\frac{\\|x - y\\|^{2}}{2\\sigma^{2}}\\right)$. El valor del par√°metro `sigma` controla la escala de afinidad entre puntos. Un **sigma alto** indica afinidad suave (m√°s conexiones). En cambio, un **sigma bajo** es sin√≥nimo de afinidad muy localizada (grupos m√°s peque√±os y definidos). En este caso ser√≠a pequ√±o por lo que la afinidad est√° muy localizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metro sigma del kernel RBF utilizado para construir la matriz de afinidad\n",
    "NHC_spec@kernelf@kpar$sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5ccff",
   "metadata": {},
   "source": [
    "+ **`withinss`**: Es un vector num√©rico que contiene la Suma de cuadrados dentro de cl√∫steres (WSS) pero en el espacio espectral, no en el espacio original. **Midiendo la cohesi√≥n interna** de cada cl√∫ster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suma de cuadrados dentro de cada cl√∫ster en el espacio original\n",
    "NHC_spec@withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171614cb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5DADE2\"><b>Apartado 5</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fce8e",
   "metadata": {},
   "source": [
    "En esta secci√≥n se determinar√° el **n√∫mero √≥ptimo de cl√∫sters** para los modelos de clustering no jer√°rquico estimados previamente. El objetivo es encontrar el m√©todo de asociaci√≥n y el n√∫mero de clusters par ala base de datos.\n",
    "\n",
    "Para cada una de estos m√©todos se aplicar√°n **tres criterios distintos de selecci√≥n de cl√∫sters**, con una funci√≥n propia ya que no esta implementada en `R`, con el fin de obtener una recomendaci√≥n robusta y comparativa:\n",
    "\n",
    "1. **M√©todo del codo (traceW)**: analiza la disminuci√≥n de la suma de inercias intra-cl√∫ster (suma de las distancias al cuadrado de cada una de las observaciones al respectivo centro del cl√∫ster) a medida que aumenta el n√∫mero de grupos. El n√∫mero √≥ptimo de cl√∫sters se identifica en el punto donde la reducci√≥n deja de ser significativa. Siendo el n√∫mero √≥ptimo de cl√∫steres q el que poseea al valor de k para el cu√°l se presente una ca√≠da repentina y m√°s grande en el WSS.\n",
    "\n",
    "2. **Estad√≠stico de Gap**: compara la compactaci√≥n observada con la que cabr√≠a esperar en configuraciones aleatorias, seleccionando el n√∫mero de cl√∫sters para el cual la separaci√≥n entre ambas es m√°xima.\n",
    "\n",
    "3. **Silueta promedio**: eval√∫a simult√°neamente la cohesi√≥n interna y la separaci√≥n entre cl√∫sters; se elige el n√∫mero de grupos que maximiza la calidad global de la partici√≥n. La silueta para una observaci√≥n determinada $i$ es una <u>**combinaci√≥n de la cohesi√≥n</u> $a_i$ y $b_i$** definida como: $s_i = \\frac{b_i - a_i}{\\max\\{a_i,\\, b_i\\}}$. Esto es equivalente a:\n",
    "\n",
    "\\begin{cases}\n",
    "1 - \\dfrac{a_i}{b_i}, & \\text{si } a_i < b_i,\\\\[6pt]\n",
    "0, & \\text{si } a_i = b_i,\\\\[6pt]\n",
    "\\dfrac{b_i}{a_i} - 1, & \\text{si } a_i > b_i.\n",
    "\\end{cases}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80991b3",
   "metadata": {},
   "source": [
    "Ya que la funci√≥n es propia se dar√° una explicaci√≥n de lo que hace.\n",
    "La funci√≥n `NHC_elbow` constituye una herramienta integrada para aplicar el m√©todo del codo en varios algoritmos de clustering no jer√°rquico: **k-means, Gaussian Mixture Models (GMM), Spectral Clustering y DBSCAN**. Su estructura est√° dise√±ada para garantizar reproducibilidad, control del rango de b√∫squeda y comparabilidad entre m√©todos. Para ello, la funci√≥n comienza verificando y cargando autom√°ticamente las librer√≠as necesarias, transformando los datos en formato matricial y fijando una semilla. Posteriormente define una funci√≥n auxiliar que calcula la *Within-Cluster Sum of Squares (WSS)*, magnitud clave para el m√©todo del codo en todos los algoritmos basados en\n",
    "k.\n",
    "\n",
    "El procedimiento para cada m√©todo se adapta a sus particularidades. En **k-means**, la funci√≥n emplea `NbClust` para obtener la WSS cl√°sica (tracew) y sugerir un n√∫mero √≥ptimo de cl√∫steres. En **GMM** y **Spectral Clustering**, donde la formulaci√≥n no incluye directamente WSS, se calcula una WSS aproximada a partir de las particiones asignadas a cada valor de k. En el caso de **DBSCAN**, donde no existe par√°metro k, se estima el \"codo\" analizando la distancia al minPts-√©simo vecino m√°s cercano y evaluando c√≥mo var√≠a el n√∫mero de cl√∫steres y el ruido para una rejilla de valores de `eps`.\n",
    "\n",
    "Finalmente, la funci√≥n organiza todos los resultados en una lista estructurada y genera **dos tablas comparativas**: una para los m√©todos basados en k y otra para DBSCAN. Estas tablas permiten observar de forma conjunta c√≥mo evoluciona la WSS o el n√∫mero de cl√∫steres seg√∫n el par√°metro estudiado, facilitando la selecci√≥n del modelo m√°s adecuado. Esta integraci√≥n en un √∫nico flujo de trabajo permite evaluar cuatro algoritmos no jer√°rquicos mediante un marco com√∫n y sistem√°tico, lo que mejora la interpretaci√≥n y la toma de decisiones en el an√°lisis de cl√∫steres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "# Funci√≥n general para aplicar el \"m√©todo del codo\" a:\n",
    "#  - k-means (usando NbClust y tracew)\n",
    "#  - GMM (Mclust, WSS aproximada)\n",
    "#  - Spectral Clustering (specc, WSS aproximada)\n",
    "#  - DBSCAN (kNN distance para elegir eps)\n",
    "#--------------------------------------------------------------\n",
    "NHC_elbow = function(data, k_min = 2, k_max = 10, eps_grid = seq(0.4, 1.2, by = 0.05), minPts_dbscan = 3, seed = 123){\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 0) Comprobaciones y carga de librer√≠as\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Aseguramos que 'data' sea una matriz num√©rica\n",
    "data = as.matrix(data)\n",
    "\n",
    "# Vector de valores de k a evaluar\n",
    "k_values = k_min:k_max\n",
    "\n",
    "# Establecemos semilla de reproducibilidad\n",
    "set.seed(seed)\n",
    "\n",
    "libs_necesarias = c(\"NbClust\", \"mclust\", \"kernlab\", \"dbscan\", \"cluster\", \"knitr\")\n",
    "\n",
    "for (lib in libs_necesarias) {\n",
    "  if (!requireNamespace(lib, quietly = TRUE)) {\n",
    "    install.packages(lib)\n",
    "  }\n",
    "  library(lib, character.only = TRUE)\n",
    "}\n",
    "\n",
    "# Cargamos librer√≠as necesarias\n",
    "library(NbClust)   # Para k-means + tracew\n",
    "library(mclust)    # Para GMM\n",
    "library(kernlab)   # Para Spectral Clustering\n",
    "library(dbscan)    # Para DBSCAN\n",
    "library(cluster)   # Para funciones de distancia\n",
    "library(knitr)     # Para realizar la tabla comparativa\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 1) Funci√≥n auxiliar: calcula WSS dada una partici√≥n\n",
    "#     x  : matriz de datos (n x p)\n",
    "#     cl : vector de cl√∫steres (longitud n)\n",
    "#----------------------------------------------------------\n",
    "wss_clusters = function(x, cl) {\n",
    "\n",
    "    # Para cada cl√∫ster j calculamos la suma de cuadrados\n",
    "    grupos = split.data.frame(x, cl)\n",
    "\n",
    "    # Eliminamos posibles grupos vac√≠os (por seguridad)\n",
    "    grupos = grupos[sapply(grupos, nrow) > 0]\n",
    "\n",
    "    # Aplica una funci√≥n a cada cl√∫ster de la lista 'grupos'\n",
    "    wss_vec = vapply(grupos, function(g) {\n",
    "      centro = colMeans(g) # Calcula el centroide del cl√∫ster (media por columnas)\n",
    "      sum(rowSums((g - centro)^2)) # Calcula la WSS del cl√∫ster: suma de distancias cuadr√°ticas de cada punto al centroide\n",
    "    }, numeric(1)) # Especifica que cada iteraci√≥n devuelve un √∫nico n√∫mero (WSS de un cl√∫ster)\n",
    "\n",
    "    # Suma las WSS de todos los cl√∫steres para obtener la WSS total\n",
    "    sum(wss_vec)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 2) K-MEANS + NBCLUST (m√©todo del codo \"cl√°sico\" con tracew)\n",
    "#----------------------------------------------------------\n",
    "wss_km = NbClust(data = data, distance = \"euclidean\", min.nc = k_min, max.nc = k_max, method = \"kmeans\", index = \"tracew\")\n",
    "\n",
    "# 'All.index' contiene el √≠ndice tracew (WSS) para cada k\n",
    "tracew_km = wss_km$All.index\n",
    "\n",
    "# Extraemos el n√∫mero √≥ptimo de cl√∫steres seg√∫n NbClust\n",
    "best_km = as.numeric(wss_km$Best.nc[1])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 3) GMM (Mclust) - WSS aproximada para cada G = k\n",
    "#----------------------------------------------------------\n",
    "wss_gmm = numeric(length(k_values))\n",
    "\n",
    "for (i in seq_along(k_values)) {\n",
    "  k = k_values[i]\n",
    "\n",
    "  # Ajustamos el modelo de mezclas gaussianas con G = k\n",
    "  gmm_k = Mclust(data = data, G = k)\n",
    "\n",
    "  # Clasificaci√≥n m√°xima a posteriori (partici√≥n dura)\n",
    "  cl = gmm_k$classification\n",
    "\n",
    "  # Calculamos WSS \"tipo k-means\" a partir de la partici√≥n\n",
    "  wss_gmm[i] = wss_clusters(data, cl)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 4) SPECTRAL CLUSTERING - WSS aproximada para cada k\n",
    "#----------------------------------------------------------\n",
    "wss_spectral = numeric(length(k_values))\n",
    "\n",
    "for (i in seq_along(k_values)) {\n",
    "  k = k_values[i]\n",
    "\n",
    "  # Ajustamos Spectral Clustering con k cl√∫steres\n",
    "  spec_k = specc(x = data, centers = k, kernel = \"rbfdot\")\n",
    "\n",
    "  # Asignaci√≥n de cl√∫steres (vector de longitud n)\n",
    "  cl = spec_k@.Data\n",
    "\n",
    "  # Calculamos WSS \"tipo k-means\"\n",
    "  wss_spectral[i] = wss_clusters(data, cl)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 5) DBSCAN - \"codo\" de distancias k-NN para eps\n",
    "#----------------------------------------------------------\n",
    "# Distancias al minPts-√©simo vecino m√°s cercano (para kNNdistplot)\n",
    "kNN_dists = kNNdist(data, k = minPts_dbscan)\n",
    "kNN_dists = sort(kNN_dists)  # ordenamos para visualizar el \"codo\"\n",
    "\n",
    "# Evaluamos una rejilla de eps para ver n√∫mero de cl√∫steres y ruido\n",
    "n_clusters_db = numeric(length(eps_grid))\n",
    "n_noise_db    = numeric(length(eps_grid))\n",
    "\n",
    "for (i in seq_along(eps_grid)) {\n",
    "  eps_i = eps_grid[i]\n",
    "\n",
    "  # Ajustamos DBSCAN para cada eps de la rejilla\n",
    "  db_i = dbscan(data, eps = eps_i, minPts = minPts_dbscan)\n",
    "\n",
    "  # N√∫mero de cl√∫steres distintos de 0 (excluimos ruido)\n",
    "  n_clusters_db[i] = length(setdiff(unique(db_i$cluster), 0))\n",
    "\n",
    "  # N√∫mero de puntos etiquetados como ruido (cluster = 0)\n",
    "  n_noise_db[i] = sum(db_i$cluster == 0)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 6) Construimos tablas resumen comparativas\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Tabla comparativa para m√©todos basados en k (k-means, GMM, Spectral)\n",
    "# Intentamos alinear tracew_km con k_values usando los nombres (por si NbClust devuelve nombres de k)\n",
    "tracew_km_num = as.numeric(tracew_km[match(k_values, names(tracew_km))])\n",
    "\n",
    "tabla_k = data.frame(\n",
    "  k                   = k_values,       # N√∫mero de cl√∫steres\n",
    "  tracew_kmeans       = tracew_km_num,  # Suma de cuadrados intra-cl√∫ster seg√∫n NbClust (k-means)\n",
    "  wss_gmm             = wss_gmm,        # WSS aproximada para GMM\n",
    "  wss_spectral        = wss_spectral    # WSS aproximada para Spectral Clustering\n",
    ")\n",
    "\n",
    "# Tabla comparativa para DBSCAN en funci√≥n de eps\n",
    "tabla_dbscan = data.frame(\n",
    "  eps        = eps_grid,      # Valores de eps evaluados\n",
    "  n_clusters = n_clusters_db, # N√∫mero de cl√∫steres (excluyendo ruido) para cada eps\n",
    "  n_noise    = n_noise_db     # N√∫mero de puntos ruido para cada eps\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 7) Devolvemos resultados en una lista estructurada\n",
    "#----------------------------------------------------------\n",
    "resultados = list(\n",
    "  kmeans = list(\n",
    "    nbclust_obj = wss_km,        # objeto completo de NbClust\n",
    "    tracew      = tracew_km,    # √≠ndice tracew para cada k\n",
    "    best_k      = best_km     # n√∫mero √≥ptimo de cl√∫steres seg√∫n NbClust\n",
    "  ),\n",
    "  gmm = list(\n",
    "    k_values = k_values,        # valores de k evaluados\n",
    "    wss      = wss_gmm          # WSS aproximada para cada k\n",
    "  ),\n",
    "  spectral = list(\n",
    "    k_values = k_values,        # valores de k evaluados\n",
    "    wss      = wss_spectral     # WSS aproximada para cada k\n",
    "  ),\n",
    "  dbscan = list(\n",
    "    minPts     = minPts_dbscan, # minPts utilizado en DBSCAN\n",
    "    kNN_dists  = kNN_dists,     # distancias k-NN ordenadas (para el \"codo\" de eps)\n",
    "    eps_grid   = eps_grid,      # rejilla de eps evaluada\n",
    "    n_clusters = n_clusters_db, # n√∫mero de cl√∫steres (excluyendo ruido) para cada eps\n",
    "    n_noise    = n_noise_db     # n√∫mero de puntos ruido para cada eps\n",
    "  ),\n",
    "  tablas = list(\n",
    "    k_methods = tabla_k,        # tabla comparativa k-means, GMM, Spectral\n",
    "    dbscan    = tabla_dbscan   # tabla comparativa para DBSCA\n",
    "    )\n",
    "  )\n",
    "\n",
    "\n",
    "# Devuelve el resultado final\n",
    "return(resultados)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la funci√≥n NHC_elbow sobre nuestra base de datos estandarizada\n",
    "res_elbow = NHC_elbow(datos_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa478bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida con la funci√≥n 'NHC_elbow' (vista completa)\n",
    "show(res_elbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39722c6d",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** El objeto resultante contiene  5 elemnetos que resumen los resultados de los cuatro m√©todos de *clustering* no jer√°rquico aplicados mediante la funci√≥n `NHC_elbow`. En primer lugar, dentro del apartado **`kmeans`**, observamos los valores del √≠ndice **tracew** para $k = 2,\\dots,10$ (`nbclust_obj$All.index`), donde la inercia intra‚Äìcl√∫ster disminuye de forma progresiva. El m√©todo `NbClust` selecciona **$k = 4$** como n√∫mero √≥ptimo de cl√∫steres (`best_k`), tambi√©n se le puede a√±adir el valor correspondiente a ese n√∫mero √≥ptimo (`nbclust_obj$Best.nc`), proporcionando adem√°s la partici√≥n completa de las observaciones (`nbclust_obj$Best.partition`). Esta coherencia entre la ca√≠da pronunciada de tracew y la elecci√≥n autom√°tica indica que el m√©todo encuentra una estructura relativamente clara para cuatro grupos.\n",
    "\n",
    "En cuanto al apartado **`gmm`**, se muestran los valores de la WSS aproximada (`wss`)para cada $k$ (`k_values`) bajo modelos de mezclas gaussianas. En este caso, los valores no siguen un patr√≥n estrictamente decreciente porque la WSS se calcula a partir de una clasificaci√≥n dura derivada del modelo probabil√≠stico. Aun as√≠, se observa cierta estabilidad en torno a $k = 2,3$ y $k = 10$, mientras que otros valores presentan incrementos debidos a ajustes de mezcla m√°s complejos.\n",
    "\n",
    "\n",
    "En **`spectral`**, la WSS aproximada (`wss`) muestra un comportamiento similar: cierta variabilidad en funci√≥n de $k$, con valores relativamente moderados entre $k=2$ y $k=10$ (`k_values`), aunque sin un \"codo\" claro. Esto indica que, para estos datos, ni GMM ni Spectral Clustering muestran una estructura de cl√∫steres tan marcada como la identificada por k-means.\n",
    "\n",
    "Finalmente, el apartado **`dbscan`** refleja que, con tres puntos para formar un cluster (`minPts`), ninguna de las opciones de $\\epsilon$ evaluadas (`eps_grid`) genera cl√∫steres excepto en la √∫ltima: todos los casos presentan **0 cl√∫steres** menos el √∫ltimo (`n_clusters`) por lo que tendran **18 puntos de ruido**excepto el √∫ltimo (`n_noise`), lo que implica que los datos no presentan densidades suficientemente diferenciadas para este m√©todo bajo dichos par√°metros, las distancias se pueden observar en `kNN_dits`.\n",
    "\n",
    "Las dos tablas finales permiten comparar visualmente estos resultados: la tabla de m√©todos basados en $k$ confirma la tendencia decreciente esperada de tracew en k-means y la variabilidad de las WSS en GMM y Spectral; mientras que la tabla espec√≠fica para DBSCAN resume la incapacidad del m√©todo para encontrar grupos en este conjunto de datos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483113f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos los resultados de los 4 m√©todos no jer√°rquicos\n",
    "# Extraemos los resultados obtenidos\n",
    "tabla_k = res_elbow$tablas$k_methods\n",
    "tabla_dbscan = res_elbow$tablas$dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a74c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la librer√≠a 'knitr'\n",
    "require(knitr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d88b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la tabla comparativa de los m√©todos k-means, DBSCAN y GMM\n",
    "kable(tabla_k, caption = \"Resumen de √≠ndices para k-means, GMM y Spectral Clustering\", digits = 3, align = \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08810b",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** A partir de la tabla comparativa de los √≠ndices obtenidos por k-means, GMM y Spectral Clustering, se observar diversos  comportamiento de cada m√©todo.\n",
    "\n",
    "En primer lugar, **k-means muestra un patr√≥n claramente decreciente en tracew**, pasando de aproximadamente 50.7 en $k=2$ a 8.1 en $k=10$. La ca√≠da m√°s pronunciada ocurre entre $k=3$ y $k=4$, lo que sugiere que el m√©todo identifica una estructura bien definida en torno a **4 grupos**. A partir de ese punto, las mejoras son m√°s moderadas, coherentes con un \"codo\" en torno a $k=4$.\n",
    "\n",
    "Por el contrario, tanto **GMM** como **Spectral Clustering** presentan **√≠ndices de dispersi√≥n m√°s irregulares**. En GMM, los valores de WSS no exhiben un descenso progresivo; de hecho, algunos valores aumentan de forma notable (por ejemplo, en $k=4$ y $k=8$), lo que indica que las particiones generadas por los modelos de mezcla no producen agrupaciones m√°s compactas conforme aumenta $k$. Spectral Clustering muestra un patr√≥n similar, con fluctuaciones marcadas y sin evidencias de un punto de inflexi√≥n claro.\n",
    "\n",
    "Comparando los tres m√©todos, se observa que **ning√∫n m√©todo basado en WSS (GMM o Spectral) sugiere un n√∫mero √≥ptimo evidente de cl√∫steres**, mientras que **k-means s√≠ identifica una estructura relativamente estable**, apoyada en una reducci√≥n clara de la inercia hasta $k=4$. Esto sugiere que, para estos datos, k-means captura mejor la organizaci√≥n subyacente, mientras que los m√©todos basados en modelos probabil√≠sticos o en la estructura espectral del grafo no logran detectar agrupaciones con la misma nitidez.\n",
    "\n",
    "Aunque todavia que queda el m√©todo DBSCAN que se estudiar√° a continuaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66bf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la tabla comparativa del m√©todo DBSCAN\n",
    "kable(tabla_dbscan, caption = \"Resumen de resultados para DBSCAN seg√∫n eps\", digits = 3, align = \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd89b92",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-**  \n",
    "El comportamiento de DBSCAN en este conjunto confirma que el par√°metro $eps$ controla de forma decisiva la estructura que el algoritmo es capaz de detectar. A partir de los resultados observados pueden destacarse varios puntos:\n",
    "\n",
    "* **Sensibilidad extrema a la densidad:** Para valores bajos y moderados de $eps$ (entre 0.40 y 1.15), el algoritmo no identifica ning√∫n cl√∫ster y clasifica todos los puntos como ruido. Esto indica que, bajo criterios estrictos de densidad, los datos no presentan regiones suficientemente compactas como para formar grupos.\n",
    "\n",
    "* **Umbral cr√≠tico alrededor de $eps \\approx 1.20$:** S√≥lo cuando el radio de vecindad aumenta lo suficiente, DBSCAN detecta la presencia de **2 cl√∫steres** y reduce significativamente la cantidad de puntos marcados como ruido (de 18 a 11). Esto refleja que la estructura del dataset no es densa, sino m√°s bien dispersa, por lo que s√≥lo agrupamientos ‚Äúamplios‚Äù cumplen los requisitos de densidad.\n",
    "\n",
    "* **Estabilidad del ruido:** El n√∫mero de puntos considerados ruido permanece constante (18) para todos los valores hasta $eps=1.15$, lo cual sugiere que el patr√≥n espacial carece de densidades locales destacables en ese rango.\n",
    "\n",
    "En resumen, el an√°lisis indica que **√∫nicamente con un $eps$ suficientemente grande** aparece una estructura de **2 cl√∫steres**, lo que sugiere que el dataset contiene grupos amplios pero poco densos. La elecci√≥n de $eps$ deber√≠a de cambiara a valores superiores quiz√° una orquilla de 1.20 a 1.75 ser√≠a m√°s obtima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd0071",
   "metadata": {},
   "source": [
    "Ahora se mostr√° en gr√°fica los valores para que se pueda observar de forma gr√°fica el \"codo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5561c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librer√≠as necesarias para llevar a cabo la representaci√≥n gr√°fica\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(tidyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la tabla de m√©todos basados en k\n",
    "tabla_k = res_elbow$tablas$k_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos a formato largo para poder usar ggplot c√≥modamente\n",
    "tabla_k_long = tabla_k %>%\n",
    "\n",
    "# Cambiamos de formato \"ancho\" a \"largo\"\n",
    "pivot_longer(\n",
    "    cols = c(tracew_kmeans, wss_gmm, wss_spectral), # Indicamos qu√© columnas queremos ‚Äúdesapilar‚Äù (las medidas de WSS)\n",
    "    names_to = \"metodo\",                            # El nombre de la nueva columna que contendr√° el nombre del m√©todo (antes era el nombre de la variable)\n",
    "    values_to = \"wss\"                               # El nombre de la nueva columna que contendr√° los valores num√©ricos de esas medidas\n",
    "  ) %>%\n",
    "\n",
    "# Modificamos o creamos columnas en el data.frame resultante\n",
    "mutate(\n",
    "  metodo = recode(                                  # Reescribimos los valores de la columna 'metodo' para que sean m√°s legibles\n",
    "    metodo,\n",
    "    \"tracew_kmeans\" = \"k-means (tracew)\",           # Cambiamos la etiqueta t√©cnica 'tracew_kmeans' por una descripci√≥n m√°s clara\n",
    "    \"wss_gmm\"       = \"GMM (WSS aprox.)\",           # Cambiamos 'wss_gmm' por 'GMM (WSS aprox.)'\n",
    "    \"wss_spectral\"  = \"Spectral (WSS aprox.)\"       # Cambiamos 'wss_spectral' por 'Spectral (WSS aprox.)'\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd298198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos gr√°ficamente el m√©todo del codo\n",
    "ggplot(tabla_k_long, aes(x = k, y = wss, color = metodo)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 2) +\n",
    "  labs(title = \"M√©todo del codo para k-means, GMM y Spectral Clustering\",\n",
    "       x     = \"N√∫mero de cl√∫steres (k)\",\n",
    "       y     = \"WSS / tracew (variabilidad intra-cl√∫ster)\",\n",
    "       color = \"M√©todo\") +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65e28b",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-**  \n",
    "El gr√°fico compara la evoluci√≥n de la variabilidad intra‚Äìcl√∫ster (WSS o su aproximaci√≥n) para **k-means**, **GMM** y **Spectral Clustering** conforme aumenta el n√∫mero de cl√∫steres $k$.  \n",
    "\n",
    "En el caso de **k-means**, se observa una **disminuci√≥n muy marcada entre $k=3$ y $k=4$**, seguida de mejoras cada vez m√°s peque√±as para valores superiores de $k$. Este comportamiento es caracter√≠stico de un **punto de codo** claro en **$k=4$**, donde a√±adir m√°s cl√∫steres no reduce de forma significativa la variabilidad interna.\n",
    "\n",
    "Por el contrario, las curvas de **GMM** y **Spectral Clustering** se mantienen **mucho m√°s aleatoria**, con oscilaciones pero sin un descenso abrupto despu√©s de los primeros valores de $k$. Esto indica que estos m√©todos **no obtienen una ganancia sustancial** al incrementar el n√∫mero de cl√∫steres, por lo que el criterio del codo no resulta tan concluyente para ellos.\n",
    "\n",
    "En conjunto, el comportamiento de las tres curvas sugiere que **$k=4$ es la elecci√≥n m√°s razonable**, respaldado por la se√±al clara dada por k-means y la ausencia de otra opci√≥n por **GMM** y **Spectral Clustering**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la tabla de los resultados de DBSCAN\n",
    "tabla_db = res_elbow$tablas$dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evoluci√≥n del n√∫mero de cl√∫steres seg√∫n eps\n",
    "ggplot(tabla_db, aes(x = eps, y = n_clusters)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 2) +\n",
    "  labs(title = \"DBSCAN: n√∫mero de cl√∫steres en funci√≥n de eps\",\n",
    "       x     = \"eps\",\n",
    "       y     = \"N√∫mero de cl√∫steres (sin ruido)\") +\n",
    "  scale_x_continuous(breaks = tabla_db$eps) +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e282e92",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-**  \n",
    "El gr√°fico muestra la evoluci√≥n del n√∫mero de cl√∫steres detectados por **DBSCAN** en funci√≥n del par√°metro `eps`. Para un amplio rango inicial (`eps` entre 0.40 y 1.15), el algoritmo devuelve **cero cl√∫steres v√°lidos**, lo que significa que **todos los puntos son considerados ruido** bajo estos niveles de exigencia de densidad. Esto sugiere que, con `minPts = 3`, la distancia entre puntos es demasiado grande para formar n√∫cleos densos en ese intervalo.\n",
    "\n",
    "Solo cuando `eps` alcanza valores alrededor de **1.20** aparece finalmente una estructura con **dos cl√∫steres**, lo que indica que el algoritmo empieza a considerar suficientes puntos dentro del radio para formar regiones densas coherentes.  \n",
    "\n",
    "En conjunto, el gr√°fico evidencia que el conjunto de datos **no presenta densidades suficientemente compactas** como para que DBSCAN detecte cl√∫steres en eps peque√±os o moderados. La aparici√≥n de cl√∫steres √∫nicamente en eps grandes sugiere que la estructura es **poco densa** y requiere radios amplios para conectarse, por lo que la configuraci√≥n de DBSCAN debe interpretarse con cautela en este caso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evoluci√≥n del n√∫mero de puntos ruido seg√∫n eps\n",
    "ggplot(tabla_db, aes(x = eps, y = n_noise)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 2) +\n",
    "  labs(title = \"DBSCAN: n√∫mero de puntos ruido en funci√≥n de eps\",\n",
    "       x     = \"eps\",\n",
    "       y     = \"N√∫mero de observaciones etiquetadas como ruido\") +\n",
    "  scale_x_continuous(breaks = tabla_db$eps) +\n",
    "**Interpretaci√≥n.-**\n",
    "El gr√°fico muestra la evoluci√≥n del n√∫mero de cl√∫steres detectados por **DBSCAN** en funci√≥n del par√°metro `eps`. Para un amplio rango inicial (`eps` entre 0.40 y 1.15), el algoritmo devuelve **cero cl√∫steres v√°lidos**, lo que significa que **todos los puntos son considerados ruido** bajo estos niveles de exigencia de densidad. Esto sugiere que, con `minPts = 3`, la distancia entre puntos es demasiado grande para formar n√∫cleos densos en ese intervalo.\n",
    "\n",
    "Solo cuando `eps` alcanza valores alrededor de **1.20** aparece finalmente una estructura con **dos cl√∫steres**, lo que indica que el algoritmo empieza a considerar suficientes puntos dentro del radio para formar regiones densas coherentes.\n",
    "\n",
    "En conjunto, el gr√°fico evidencia que el conjunto de datos **no presenta densidades suficientemente compactas** como para que DBSCAN detecte cl√∫steres en eps peque√±os o moderados. La aparici√≥n de cl√∫steres √∫nicamente en eps grandes sugiere que la estructura es **poco densa** y requiere radios amplios para conectarse, por lo que la configuraci√≥n de DBSCAN debe interpretarse con cautela en este caso.\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1a57a",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretaci√≥n.-**  \n",
    "La gr√°fica muestra la evoluci√≥n del n√∫mero de puntos etiquetados como **ruido** por DBSCAN en funci√≥n del par√°metro `eps`. Para todo el rango inicial (`eps` entre 0.40 y 1.15), el algoritmo considera **a las 18 observaciones como ruido**, lo cual indica que **no existe suficiente densidad local** para formar cl√∫steres con `minPts = 3` en ese intervalo.\n",
    "\n",
    "Solo cuando `eps` alcanza aproximadamente **1.20** el n√∫mero de puntos ruido cae bruscamente hasta **11**, lo que significa que algunos puntos empiezan a encontrarse dentro de radios suficientemente amplios como para cumplir el criterio de densidad y formar un par de cl√∫steres v√°lidos.\n",
    "\n",
    "En conjunto, el gr√°fico confirma que el conjunto de datos presenta una **densidad muy baja o muy dispersa**, de modo que DBSCAN no identifica regiones densas hasta valores de `eps` relativamente grandes. Esto refuerza la idea de que, para este dataset, DBSCAN solo ofrece soluciones interpretables cuando se permite un radio amplio que conecte los puntos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b39ca",
   "metadata": {},
   "source": [
    "Tras realizar el an√°lisis del criterio del codo se realizar√° el criterio delestadistico GAP, como la funci√≥n es propia se explicar√° que realiza la funci√≥n.\n",
    "\n",
    "La funci√≥n NHC_gap sustituye el an√°lisis anterior basado en el criterio del codo, por el Estad√≠stico GAP, que es una medida m√°s robusta y formal para determinar el n√∫mero √≥ptimo de cl√∫steres. Mientras que el criterio del codo se basaba en observar visualmente la reducci√≥n de la variabilidad intra‚Äìcl√∫ster, esta funci√≥n calcula el valor GAP mediante bootstrap, comparando la dispersi√≥n real de los datos con la dispersi√≥n esperada bajo una distribuci√≥n de referencia uniforme. Esto permite seleccionar el n√∫mero de cl√∫steres no solo por el \"codo\", sino mediante un criterio estad√≠stico con soporte te√≥rico.\n",
    "\n",
    "Adem√°s, la funci√≥n generaliza el an√°lisis a tres metodolog√≠as distintas: k-means, Gaussian Mixture Models (GMM) y Spectral Clustering, creando wrappers personalizados para que todos puedan evaluarse con clusGap. Para cada m√©todo, la funci√≥n construye una tabla comparativa de valores GAP entre diferentes valores de $k$, lo que permite contrastar c√≥mo cada algoritmo estructura los datos y si coinciden o divergen en la elecci√≥n del n√∫mero √≥ptimo de cl√∫steres. Esto supone una mejora respecto a la funci√≥n anterior del codo, que solo evaluaba la variabilidad interna sin marco de comparaci√≥n entre m√©todos.\n",
    "\n",
    "Finalmente, la funci√≥n mantiene un tratamiento especial para DBSCAN, ya que el Estad√≠stico GAP no es aplicable a algoritmos basados en densidad. En lugar de forzar un criterio inadecuado, la funci√≥n implementa un an√°lisis sistem√°tico del par√°metro eps, registrando el n√∫mero de cl√∫steres y el n√∫mero de puntos ruido para distintos valores. Con ello se obtiene una visi√≥n clara de c√≥mo evoluciona la estructura de densidad, complementando el enfoque basado en $k$ de los m√©todos anteriores. El resultado final es un procedimiento unificado, m√°s riguroso y comparativo, que supera ampliamente las limitaciones del an√°lisis previo basado √∫nicamente en el codo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "# Funci√≥n general para estimar el Estad√≠stico GAP en:\n",
    "#  - k-means (usando clusGap)\n",
    "#  - GMM (Mclust + wrapper para clusGap)\n",
    "#  - Spectral Clustering (specc + wrapper para clusGap)\n",
    "#  - DBSCAN (Se mantiene an√°lisis de eps, ya que Gap no aplica a densidad sin k fijo)\n",
    "#--------------------------------------------------------------\n",
    "NHC_gap = function(data, k_min = 2, k_max = 10, B = 50, eps_grid = seq(0.4, 1.2, by = 0.05), minPts_dbscan = 3, seed = 123){\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 0) Comprobaciones y carga de librer√≠as\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Aseguramos que 'data' sea una matriz num√©rica\n",
    "data = as.matrix(data)\n",
    "\n",
    "# Vector de valores de k a evaluar\n",
    "k_values = k_min:k_max\n",
    "\n",
    "# Establecemos semilla de reproducibilidad\n",
    "set.seed(seed)\n",
    "\n",
    "libs_necesarias = c(\"mclust\", \"kernlab\", \"dbscan\", \"cluster\")\n",
    "\n",
    "for (lib in libs_necesarias) {\n",
    "  if (!requireNamespace(lib, quietly = TRUE)) {\n",
    "    install.packages(lib)\n",
    "  }\n",
    "  library(lib, character.only = TRUE)\n",
    "}\n",
    "\n",
    "# Cargamos librer√≠as necesarias\n",
    "library(cluster)   # Para clusGap (k-means)\n",
    "library(mclust)    # Para GMM\n",
    "library(kernlab)   # Para Spectral Clustering\n",
    "library(dbscan)    # Para DBSCAN\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 1) Definici√≥n de Wrappers (Funciones auxiliares para clusGap)\n",
    "#    clusGap necesita funciones que acepten (x, k) y devuelvan\n",
    "#    una lista con el componente $cluster\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Wrapper para GMM\n",
    "gmm_fn = function(x, k){\n",
    "  model = Mclust(x, G = k, verbose = FALSE)\n",
    "  list(cluster = model$classification)\n",
    "}\n",
    "\n",
    "# Wrapper para Spectral Clustering\n",
    "spec_fn = function(x, k){\n",
    "  # Usamos tryCatch por si falla la convergencia en alguna iteraci√≥n\n",
    "  out = tryCatch({\n",
    "    res = specc(x, centers = k, kernel = \"rbfdot\")\n",
    "    list(cluster = res@.Data)\n",
    "  }, error = function(e) {\n",
    "    # En caso de error, devolvemos un cluster aleatorio (fallback)\n",
    "    list(cluster = sample(1:k, nrow(x), replace = TRUE))\n",
    "  })\n",
    "  return(out)\n",
    "}\n",
    "\n",
    "# Wrapper para K-means (aseguramos nstart para estabilidad)\n",
    "kmeans_fn = function(x, k){\n",
    "  kmeans(x, centers = k, nstart = 25)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 2) K-MEANS - Estad√≠stico GAP\n",
    "#----------------------------------------------------------\n",
    "# Calculamos el Gap statistic con B iteraciones de bootstrap\n",
    "gap_km_obj = clusGap(data, FUNcluster = kmeans_fn, K.max = k_max, B = B)\n",
    "\n",
    "# Calculamos el Gap statistic con B iteraciones de bootstrap\n",
    "# La tabla de clusGap empieza en k=1, filtramos por filas\n",
    "gap_km_values = gap_km_obj$Tab[k_values, \"gap\"]\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 3) GMM - Estad√≠stico GAP\n",
    "#----------------------------------------------------------\n",
    "# Nota: Esto puede tardar dependiendo del tama√±o de datos y B\n",
    "# Calculamos el Gap statistic con B iteraciones de bootstrap\n",
    "gap_gmm_obj = clusGap(data, FUNcluster = gmm_fn, K.max = k_max, B = B)\n",
    "\n",
    "# Calculamos el Gap statistic con B iteraciones de bootstrap\n",
    "# La tabla de clusGap empieza en k=1, filtramos por filas\n",
    "gap_gmm_values = gap_gmm_obj$Tab[k_values, \"gap\"]\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 4) SPECTRAL CLUSTERING - Estad√≠stico GAP\n",
    "#----------------------------------------------------------\n",
    "# Nota: Esto puede tardar dependiendo del tama√±o de datos y B\n",
    "# Calculamos el Gap statistic con B iteraciones de bootstrap\n",
    "gap_spec_obj = clusGap(data, FUNcluster = spec_fn, K.max = k_max, B = B)\n",
    "\n",
    "# Calculamos el Gap statistic con B iteraciones de bootstrap\n",
    "# La tabla de clusGap empieza en k=1, filtramos por filas\n",
    "gap_spec_values = gap_spec_obj$Tab[k_values, \"gap\"]\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 5) DBSCAN - An√°lisis de sensibilidad (eps)\n",
    "#    El estad√≠stico Gap no es aplicable directamente porque\n",
    "#    DBSCAN no fija k. Mantenemos el an√°lisis de eps/ruido.\n",
    "#----------------------------------------------------------\n",
    "# Distancias al minPts-√©simo vecino (para visualizaci√≥n)\n",
    "kNN_dists = kNNdist(data, k = minPts_dbscan)\n",
    "kNN_dists = sort(kNN_dists)\n",
    "\n",
    "# Evaluamos la rejilla de eps\n",
    "n_clusters_db = numeric(length(eps_grid))\n",
    "n_noise_db    = numeric(length(eps_grid))\n",
    "\n",
    "# Ajustamos DBSCAN para cada eps de la rejilla\n",
    "for (i in seq_along(eps_grid)) {\n",
    "  eps_i = eps_grid[i]\n",
    "  db_i = dbscan(data, eps = eps_i, minPts = minPts_dbscan)\n",
    "\n",
    "  # Clusters excluyendo ruido (0)\n",
    "  n_clusters_db[i] = length(setdiff(unique(db_i$cluster), 0))\n",
    "  n_noise_db[i]    = sum(db_i$cluster == 0)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 6) Construimos tablas resumen comparativas\n",
    "#----------------------------------------------------------\n",
    "# Tabla comparativa de valores GAP\n",
    "tabla_gap = data.frame(k = k_values, gap_kmeans = gap_km_values, gap_gmm = gap_gmm_values, gap_spectral = gap_spec_values)\n",
    "\n",
    "# Tabla comparativa para DBSCAN\n",
    "tabla_dbscan = data.frame(eps = eps_grid, n_clusters = n_clusters_db, n_noise = n_noise_db)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 7) Devolvemos resultados en lista\n",
    "#----------------------------------------------------------\n",
    "resultados = list(\n",
    "  gap_objects = list(\n",
    "    kmeans   = gap_km_obj,    # Objeto completo clusGap (contiene SE.sim, etc.)\n",
    "    gmm      = gap_gmm_obj,\n",
    "    spectral = gap_spec_obj\n",
    "  ),\n",
    "  dbscan = list(\n",
    "    minPts     = minPts_dbscan,\n",
    "    kNN_dists  = kNN_dists,\n",
    "    eps_grid   = eps_grid,\n",
    "    n_clusters = n_clusters_db,\n",
    "    n_noise    = n_noise_db\n",
    "  ),\n",
    "  tablas = list(\n",
    "    gap_methods = tabla_gap,   # Tabla resumen de valores Gap\n",
    "    dbscan      = tabla_dbscan # Tabla resumen DBSCAN\n",
    "  )\n",
    ")\n",
    "\n",
    "# Devuelve el resultado final\n",
    "return(resultados)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la funci√≥n NHC_gap sobre nuestra base de datos estandarizada (puede tardar unos minutos)\n",
    "res_gap = NHC_gap(datos_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3362b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida con la funci√≥n 'NHC_gap'\n",
    "str(res_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida con la funci√≥n 'NHC_gap' (vista completa)\n",
    "show(res_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93067a5",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** El objeto generado por la funci√≥n `NHC_gap` contiene **tres bloques de resultados**: los estad√≠sticos Gap para los m√©todos basados en $k$ (*k-means*, *GMM* y *Spectral Clustering*), el an√°lisis de sensibilidad para **DBSCAN**, y dos tablas resumen listas para an√°lisis comparativo. En conjunto, este resultado permite evaluar la estructura de cl√∫steres desde una perspectiva estad√≠stica m√°s s√≥lida que el criterio del codo, ya que compara la dispersi√≥n observada con la obtenida bajo una distribuci√≥n de referencia. Con de $50$ iteraciones bootstrap  (`B`).\n",
    "\n",
    "Dentro del apartado **`gap_objects`**, observamos que los tres m√©todos (*k-means*, *GMM* y *Spectral*) seleccionan **$k = 1$** como n√∫mero √≥ptimo de cl√∫steres bajo el criterio *firstSEmax*, ya que el valor del estad√≠stico Gap es m√°ximo en $k = 1$ y decrece conforme aumenta $k$. Esto es coherente con las tablas: en los tres m√©todos, el estad√≠stico Gap es positivo √∫nicamente en $k = 1$, y en la mayor√≠a de los casos se vuelve negativo a partir de $k = 2$, indicando que a√±adir m√°s cl√∫steres no mejora la separaci√≥n respecto al modelo nulo. Adem√°s, el patr√≥n decreciente o fluctuante de la columna `gap` en `Tab` confirma que no hay evidencia estad√≠stica de m√∫ltiples grupos bien definidos en los datos seg√∫n ninguno de los tres algoritmos.\n",
    "\n",
    "En cuanto al bloque **`dbscan`**, el an√°lisis de densidad muestra una situaci√≥n similar. Para la rejilla de valores de $\\epsilon$ definida (`eps_grid`), el m√©todo produce **0 cl√∫steres** en todos los casos salvo el √∫ltimo, donde encuentra √∫nicamente **2 cl√∫steres** (`n_clusters`) y reduce el n√∫mero de puntos de ruido a 11 (`n_noise`). Esto implica que, para la mayor parte del rango explorado, las distancias entre puntos (`kNN_dists`) no son suficientemente peque√±as como para formar regiones densas estables, la cual se definio como 3 equipos para un cluster (`minPts`) y el m√©todo no detecta estructura agregada relevante bajo los par√°metros elegidos.\n",
    "\n",
    "Finalmente, las dos tablas incluidas en **`tablas`** sintetizan estas conclusiones. La tabla **`gap_methods`** muestra que para $k = 2,\\dots,10$ todos los valores del estad√≠stico Gap son peque√±os y mayoritariamente negativos, reforzando que ninguno de los m√©todos identifica agrupamientos significativos m√°s all√° de $k = 1$. Por su parte, la tabla **`dbscan`** confirma la ausencia de cl√∫steres en la mayor√≠a de configuraciones de $\\epsilon$. En conjunto, este objeto indica que, seg√∫n el criterio Gap y el an√°lisis de densidad, **los datos no presentan una estructura clara de cl√∫steres**, o bien esta es demasiado d√©bil para ser detectada por los m√©todos considerados.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos los resultados de los 4 m√©todos no jer√°rquicos\n",
    "# Extraemos la tabla del Estad√≠stico Gap\n",
    "tabla_gap = res_gap$tablas$gap_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f58a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la tabla comparativa de los m√©todos k-means, DBSCAN y GMM (Estad√≠stico Gap)\n",
    "kable(tabla_gap, caption = \"Resumen del Estad√≠stico Gap para k-means, GMM y Spectral Clustering\", digits = 4, align = \"c\",\n",
    "      col.names = c(\"k\", \"k-means (Gap)\", \"GMM (Gap)\", \"Spectral (Gap)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183d518",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** La tabla muestra los valores del **Estad√≠stico Gap** para distintos valores de $k$ usando *k-means*, *GMM* y *Spectral Clustering*. En este caso, los valores del Gap son muy peque√±os y **no presentan m√°ximos positivos claros**, lo que indica que los datos **no muestran una estructura de cl√∫steres fuerte** seg√∫n esta m√©trica.\n",
    "\n",
    "Ninguno de los tres m√©todos exhibe un incremento sustancial del Gap que sugiera la presencia de agrupamientos bien definidos. De hecho:\n",
    "\n",
    "- En **k-means**, los valores del Gap son ligeramente positivos solo en $k=2$, y desde $k=3$ en adelante son todos negativos, indicando que a√±adir m√°s cl√∫steres **no mejora la separaci√≥n respecto al modelo nulo** ya que un valor negativo significa que es peor que una elecci√≥n aleatoria de clusters.\n",
    "- En **GMM**, ocurre algo similar: los valores oscilan en torno a cero y, aunque algunos son positivos (por ejemplo, en $k=4$ y $k=5$), no siguen un patr√≥n creciente ni definen un m√°ximo destacable.\n",
    "- En **Spectral Clustering**, los valores positivos aparecen en algunos valores de $k$ (como $k=3$, $k=4$ y $k=7$), pero no forman una tendencia consistente ni estable a trav√©s de los valores de $k$.\n",
    "\n",
    "Dado que el **criterio del Gap busca un m√°ximo bien definido**, la ausencia de un pico claro implica que el conjunto de datos **no presenta una estructura de cl√∫steres claramente identificable** bajo estos m√©todos. En consecuencia, **no hay evidencia s√≥lida para seleccionar un valor √≥ptimo de $k$**, y cualquier elecci√≥n deber√≠a basarse en otros criterios (codo y silueta promedio).\n",
    "\n",
    "En resumen, el Estad√≠stico Gap sugiere que **los datos no contienen una estructura de cl√∫steres fuerte o bien separada**, y que **ning√∫n valor de $k$ destaca como √≥ptimo** seg√∫n este criterio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la tabla comparativa del m√©todo DBSCAN (Estad√≠stico Gap)\n",
    "kable(res_gap$tablas$dbscan, caption = \"Resumen de resultados para DBSCAN seg√∫n eps (Estad√≠stico Gap)\", digits = 3, align = \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a847a",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** Los resultados muestran que **DBSCAN no identifica ning√∫n cl√∫ster** en la mayor parte del rango de valores de `eps`. Para todos los radios entre 0.40 y 1.15, el algoritmo considera que **todos los puntos son ruido**, lo que implica que la densidad local nunca es lo suficientemente alta como para formar grupos seg√∫n el criterio interno del algoritmo. Este comportamiento indica que, en este conjunto de datos, **no existen zonas de alta densidad separadas entre s√≠** bajo los valores de vecindad evaluados.\n",
    "\n",
    "Solo cuando `eps = 1.20` se detecta una estructura no trivial: aparecen **2 cl√∫steres** y el n√∫mero de puntos clasificados como ruido se reduce a 11. Esto sugiere que solo con un radio de vecindad relativamente grande, es decir, permitiendo que muchos puntos entren dentro del mismo entorno de densidad, DBSCAN es capaz de encontrar grupos cohesivos. Sin embargo, este valor elevado de `eps` tambi√©n implica que la separaci√≥n entre los cl√∫steres es d√©bil y que el algoritmo est√° forzando la uni√≥n de regiones que solo parecen densas a gran escala.\n",
    "\n",
    "En conjunto, el an√°lisis indica que **DBSCAN no detecta una estructura de densidad clara en los datos** salvo cuando se incrementa el radio de manera muy amplia. Por tanto, este algoritmo **no aporta evidencia s√≥lida de cl√∫steres bien definidos**, y confirma que los m√©todos basados en distancia o conectividad (como k-means, GMM o spectral) son m√°s adecuados para este conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18410d4c",
   "metadata": {},
   "source": [
    "Al igual que en el criterio anterior se proceder√° a graficarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d634ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la tabla de resultados GAP\n",
    "tabla_gap = res_gap$tablas$gap_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos a formato largo para ggplot\n",
    "tabla_gap_long = tabla_gap %>%\n",
    "  pivot_longer(\n",
    "    cols      = c(gap_kmeans, gap_gmm, gap_spectral),\n",
    "    names_to  = \"metodo\",\n",
    "    values_to = \"gap_stat\"\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    metodo = recode(\n",
    "      metodo,\n",
    "      \"gap_kmeans\"   = \"k-means (Gap)\",\n",
    "      \"gap_gmm\"      = \"GMM (Gap)\",\n",
    "      \"gap_spectral\" = \"Spectral (Gap)\"\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos gr√°ficamente el Estad√≠stico Gap\n",
    "ggplot(tabla_gap_long, aes(x = k, y = gap_stat, color = metodo)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 2) +\n",
    "  labs(title = \"Estad√≠stico Gap para k-means, GMM y Spectral Clustering\",\n",
    "       subtitle = \"Valores m√°s altos indican una mejor estructura de cl√∫steres\",\n",
    "       x     = \"N√∫mero de cl√∫steres (k)\",\n",
    "       y     = \"Estad√≠stico Gap\",\n",
    "       color = \"M√©todo\") +\n",
    "  scale_x_continuous(breaks = tabla_gap$k) +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ee748",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** La representaci√≥n del **estad√≠stico Gap** evidencia un patr√≥n consistente entre los tres m√©todos evaluados, destacando de forma clara que la estructura m√°s s√≥lida se alcanza en $k=4$. El m√©todo **Spectral Clustering** (l√≠nea azul) presenta su **m√°ximo global** en este punto, lo que indica que, seg√∫n la conectividad y las relaciones no lineales entre observaciones, la partici√≥n en tres grupos es la m√°s marcada. Por su parte, **GMM** muestra tambi√©n un **pico local pronunciado** en $k=3$, antes de que el valor del Gap comience a disminuir progresivamente para valores superiores de $k$.\n",
    "\n",
    "En el caso de **k-means**, exceptuando en $k=2$ los valores son negativos indicando que es mejor la selecci√≥n de clusters de forma aleatoria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la tabla de DBSCAN del objeto res_gap\n",
    "tabla_gap_db = res_gap$tablas$dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20429704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: Como DBSCAN no usa Gap directo, visualizamos la estabilidad estructural\n",
    "# Evoluci√≥n del n√∫mero de cl√∫steres seg√∫n eps\n",
    "ggplot(tabla_gap_db, aes(x = eps, y = n_clusters)) +\n",
    "  geom_line(linewidth = 0.8, color = \"darkblue\") +\n",
    "  geom_point(size = 2, color = \"darkblue\") +\n",
    "  labs(title = \"DBSCAN: Estabilidad de cl√∫steres seg√∫n eps\",\n",
    "       x     = \"eps (Radio de vecindad)\",\n",
    "       y     = \"N√∫mero de cl√∫steres formados\") +\n",
    "  scale_x_continuous(breaks = tabla_gap_db$eps) +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32daa5",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-**  \n",
    "El gr√°fico muestra la evoluci√≥n del n√∫mero de cl√∫steres detectados por **DBSCAN** en funci√≥n del par√°metro `eps`. Para un amplio rango inicial (`eps` entre 0.40 y 1.15), el algoritmo devuelve **cero cl√∫steres v√°lidos**, lo que significa que **todos los puntos son considerados ruido** bajo estos niveles de exigencia de densidad. Esto sugiere que, con `minPts = 3`, la distancia entre puntos es demasiado grande para formar n√∫cleos densos en ese intervalo.\n",
    "\n",
    "Solo cuando `eps` alcanza valores alrededor de **1.20** aparece finalmente una estructura con **dos cl√∫steres**, lo que indica que el algoritmo empieza a considerar suficientes puntos dentro del radio para formar regiones densas coherentes.  \n",
    "\n",
    "En conjunto, el gr√°fico evidencia que el conjunto de datos **no presenta densidades suficientemente compactas** como para que DBSCAN detecte cl√∫steres en eps peque√±os o moderados. La aparici√≥n de cl√∫steres √∫nicamente en eps grandes sugiere que la estructura es **poco densa** y requiere radios amplios para conectarse, por lo que la configuraci√≥n de DBSCAN debe interpretarse con cautela en este caso.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evoluci√≥n del ruido seg√∫n eps\n",
    "ggplot(tabla_gap_db, aes(x = eps, y = n_noise)) +\n",
    "  geom_line(linewidth = 0.8, color = \"firebrick\") +\n",
    "  geom_point(size = 2, color = \"firebrick\") +\n",
    "  labs(title = \"DBSCAN: Cantidad de ruido seg√∫n eps\",\n",
    "       x     = \"eps (Radio de vecindad)\",\n",
    "       y     = \"Puntos considerados ruido\") +\n",
    "  scale_x_continuous(breaks = tabla_gap_db$eps) +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767c0db",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-**  \n",
    "La gr√°fica muestra la evoluci√≥n del n√∫mero de puntos etiquetados como **ruido** por DBSCAN en funci√≥n del par√°metro `eps`. Para todo el rango inicial (`eps` entre 0.40 y 1.15), el algoritmo considera **a las 18 observaciones como ruido**, lo cual indica que **no existe suficiente densidad local** para formar cl√∫steres con `minPts = 3` en ese intervalo.\n",
    "\n",
    "Solo cuando `eps` alcanza aproximadamente **1.20** el n√∫mero de puntos ruido cae bruscamente hasta **11**, lo que significa que algunos puntos empiezan a encontrarse dentro de radios suficientemente amplios como para cumplir el criterio de densidad y formar un par de cl√∫steres v√°lidos.\n",
    "\n",
    "En conjunto, el gr√°fico confirma que el conjunto de datos presenta una **densidad muy baja o muy dispersa**, de modo que DBSCAN no identifica regiones densas hasta valores de `eps` relativamente grandes. Esto refuerza la idea de que, para este dataset, DBSCAN solo ofrece soluciones interpretables cuando se permite un radio amplio que conecte los puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92954611",
   "metadata": {},
   "source": [
    "Ya que la funci√≥n es propia se dar√° una explicaci√≥n de lo que hace. La funci√≥n NHC_elbow constituye una herramienta integrada para aplicar el m√©todo del codo en varios algoritmos de clustering no jer√°rquico: k-means, Gaussian Mixture Models (GMM), Spectral Clustering y DBSCAN. Su estructura est√° dise√±ada para garantizar reproducibilidad, control del rango de b√∫squeda y comparabilidad entre m√©todos. A partir del conjunto de datos introducido, la funci√≥n calcula m√∫ltiples particiones para un rango de valores de $k$ y para los distintos algoritmos: **k-means**, **GMM** y **Spectral Clustering**. Para cada combinaci√≥n, obtiene m√©tricas internas de validaci√≥n como el **√≠ndice de silueta**, el **√≠ndice de Dunn**, el **Davies‚ÄìBouldin**, el **Calinski‚ÄìHarabasz**, el **Gap Statistic** y otras medidas relevantes que permiten comparar la calidad de la estructura encontrada. Adem√°s, organiza todos los resultados en tablas y listas para incorporar en informes de forma inmediata sin neceisidad de generaci√≥n externa, que sintetizan el comportamiento de los m√©todos frente al n√∫mero de cl√∫steres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "# Funci√≥n general para calcular la Silueta Promedio en:\n",
    "#  - k-means (stats + cluster)\n",
    "#  - GMM (mclust + cluster)\n",
    "#  - Spectral Clustering (kernlab + cluster)\n",
    "#  - DBSCAN (dbscan + cluster)\n",
    "#--------------------------------------------------------------\n",
    "NHC_silueta = function(data, k_min = 2, k_max = 10, eps_grid = seq(0.4, 1.2, by = 0.05), minPts_dbscan = 3, seed = 123){\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 0) Comprobaciones y carga de librer√≠as\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Aseguramos que 'data' sea una matriz num√©rica\n",
    "data = as.matrix(data)\n",
    "\n",
    "# Calculamos la matriz de distancias una sola vez (necesaria para silhouette)\n",
    "# Usamos distancia Eucl√≠dea por defecto\n",
    "dist_mat = dist(data)\n",
    "\n",
    "# Vector de valores de k a evaluar\n",
    "k_values = k_min:k_max\n",
    "\n",
    "# Establecemos semilla de reproducibilidad\n",
    "set.seed(seed)\n",
    "\n",
    "libs_necesarias = c(\"mclust\", \"kernlab\", \"dbscan\", \"cluster\")\n",
    "\n",
    "for (lib in libs_necesarias) {\n",
    "  if (!requireNamespace(lib, quietly = TRUE)) {\n",
    "    install.packages(lib)\n",
    "  }\n",
    "  library(lib, character.only = TRUE)\n",
    "}\n",
    "\n",
    "# Cargamos librer√≠as necesarias\n",
    "library(cluster)   # Para la funci√≥n silhouette()\n",
    "library(mclust)    # Para GMM\n",
    "library(kernlab)   # Para Spectral Clustering\n",
    "library(dbscan)    # Para DBSCAN\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 1) Funci√≥n auxiliar: extrae silueta promedio\n",
    "#----------------------------------------------------------\n",
    "calc_sil_avg = function(cl, d_mat) {\n",
    "\n",
    "# La silueta solo se define para k >= 2\n",
    "# Si hay 1 solo grupo o todo es ruido, devuelve NA o 0\n",
    "k_enc = length(unique(cl))\n",
    "  if (k_enc < 2) {\n",
    "    return(NA)\n",
    "  } else {\n",
    "    sil_obj = silhouette(cl, d_mat)\n",
    "    # Devolvemos la media de la tercera columna (width)\n",
    "    return(summary(sil_obj)$avg.width)\n",
    "  }\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 2) K-MEANS - Silueta Promedio\n",
    "#----------------------------------------------------------\n",
    "sil_km = numeric(length(k_values))\n",
    "\n",
    "for (i in seq_along(k_values)) {\n",
    "  k = k_values[i]\n",
    "\n",
    "  # Ejecutamos k-means\n",
    "  km_res = kmeans(data, centers = k, nstart = 25)\n",
    "\n",
    "  # Calculamos silueta\n",
    "  sil_km[i] = calc_sil_avg(km_res$cluster, dist_mat)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 3) GMM (Mclust) - Silueta Promedio\n",
    "#----------------------------------------------------------\n",
    "sil_gmm = numeric(length(k_values))\n",
    "\n",
    "for (i in seq_along(k_values)) {\n",
    "  k = k_values[i]\n",
    "\n",
    "  # Ajustamos GMM\n",
    "  gmm_res = Mclust(data, G = k, verbose = FALSE)\n",
    "\n",
    "  # Clasificaci√≥n\n",
    "  cl = gmm_res$classification\n",
    "\n",
    "  # Nota: Calculamos la silueta sobre la distancia Eucl√≠dea original\n",
    "  # para hacer los m√©todos comparables en t√©rminos de compacidad geom√©trica.\n",
    "  sil_gmm[i] = calc_sil_avg(cl, dist_mat)\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 4) SPECTRAL CLUSTERING - Silueta Promedio\n",
    "#----------------------------------------------------------\n",
    "sil_spectral = numeric(length(k_values))\n",
    "\n",
    "for (i in seq_along(k_values)) {\n",
    "  k = k_values[i]\n",
    "\n",
    "  # Ajustamos Spectral\n",
    "  # Usamos tryCatch para evitar paradas si no converge\n",
    "  spec_res = tryCatch({\n",
    "    specc(data, centers = k, kernel = \"rbfdot\")\n",
    "  }, error = function(e) NULL)\n",
    "\n",
    "  if (!is.null(spec_res)) {\n",
    "    cl = spec_res@.Data\n",
    "    sil_spectral[i] = calc_sil_avg(cl, dist_mat)\n",
    "  } else {\n",
    "    sil_spectral[i] = NA\n",
    "  }\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 5) DBSCAN - Silueta Promedio por eps\n",
    "#----------------------------------------------------------\n",
    "# Para DBSCAN, calcularemos la silueta de los puntos CLASIFICADOS.\n",
    "# Opci√≥n: Excluir el ruido (cluster 0) del c√°lculo de la media,\n",
    "# ya que el ruido no forma un cl√∫ster cohesivo.\n",
    "\n",
    "sil_dbscan    = numeric(length(eps_grid))\n",
    "n_clusters_db = numeric(length(eps_grid))\n",
    "n_noise_db    = numeric(length(eps_grid))\n",
    "\n",
    "# Distancias kNN para referencia visual del codo (igual que en funciones previas)\n",
    "kNN_dists = kNNdist(data, k = minPts_dbscan)\n",
    "kNN_dists = sort(kNN_dists)\n",
    "\n",
    "for (i in seq_along(eps_grid)) {\n",
    "  eps_i = eps_grid[i]\n",
    "\n",
    "  db_res = dbscan(data, eps = eps_i, minPts = minPts_dbscan)\n",
    "  cl = db_res$cluster\n",
    "\n",
    "  # Guardamos m√©tricas estructurales\n",
    "  n_clusters_db[i] = length(setdiff(unique(cl), 0))\n",
    "  n_noise_db[i]    = sum(cl == 0)\n",
    "\n",
    "  # Calculamos silueta SOLO si hay al menos 2 cl√∫steres reales (excluyendo ruido)\n",
    "  # O si consideramos ruido como grupo, depender√° de la interpretaci√≥n.\n",
    "  # Aqu√≠: filtramos el ruido para ver la calidad de los grupos formados.\n",
    "  mask_no_ruido = cl != 0\n",
    "\n",
    "  if (length(unique(cl[mask_no_ruido])) >= 2) {\n",
    "    # Calculamos silueta solo con los datos que no son ruido\n",
    "    # Necesitamos subconjunto de la matriz de distancias tambi√©n\n",
    "    dist_subset = as.dist(as.matrix(dist_mat)[mask_no_ruido, mask_no_ruido])\n",
    "    sil_val = calc_sil_avg(cl[mask_no_ruido], dist_subset)\n",
    "    sil_dbscan[i] = sil_val\n",
    "  } else {\n",
    "    sil_dbscan[i] = NA # No aplica silueta si hay < 2 grupos\n",
    "  }\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 6) Construimos tablas resumen comparativas\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Tabla comparativa para m√©todos basados en k\n",
    "tabla_sil = data.frame(k = k_values, sil_kmeans = sil_km, sil_gmm = sil_gmm, sil_spectral = sil_spectral)\n",
    "\n",
    "# Tabla comparativa para DBSCAN\n",
    "tabla_dbscan = data.frame(eps = eps_grid, n_clusters = n_clusters_db, n_noise = n_noise_db, avg_sil_clu = sil_dbscan)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 7) Devolvemos resultados en lista estructurada\n",
    "#----------------------------------------------------------\n",
    "resultados = list(\n",
    "  metrica = \"Average Silhouette Width\",\n",
    "  dbscan_info = list(\n",
    "    minPts    = minPts_dbscan,\n",
    "    kNN_dists = kNN_dists,\n",
    "    nota      = \"La silueta de DBSCAN se calcula excluyendo puntos de ruido (cluster 0)\"\n",
    "  ),\n",
    "  tablas = list(\n",
    "    sil_methods = tabla_sil,    # Tabla resumen K-means, GMM, Spectral\n",
    "    dbscan      = tabla_dbscan  # Tabla resumen DBSCAN\n",
    "  )\n",
    ")\n",
    "\n",
    "# Devuelve el resultado final\n",
    "return(resultados)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la funci√≥n NHC_silueta sobre nuestra base de datos estandarizada\n",
    "res_silueta = NHC_silueta(datos_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d160c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida con la funci√≥n 'NHC_silueta'\n",
    "str(res_silueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5dc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n obtenida con la funci√≥n 'NHC_silueta' (vista completa)\n",
    "show(res_silueta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d87af",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** El objeto resultante `res_silueta` contiene 3 elementos principales que resumen la evaluaci√≥n basada en el Average Silhouette Width para los m√©todos de clustering considerados, incluyendo una secci√≥n espec√≠fica dedicada a DBSCAN. El primer elemento, **`metrica`**, indica claramente que la medida empleada para comparar la calidad de los agrupamientos es **\"Average Silhouette Width\"** (Anchura de Silueta Promedio), una m√©trica que eval√∫a simult√°neamente la coherencia interna de los cl√∫steres y la separaci√≥n entre ellos.\n",
    "\n",
    "En el apartado **`dbscan_info`**, se especifica el valor de `minPts = 3`, correspondiente al n√∫mero m√≠nimo de vecinos requeridos para formar un cl√∫ster en DBSCAN. A continuaci√≥n, se muestran las distancias a los 3 vecinos m√°s cercanos (`kNN_dists`), donde los valores oscilan aproximadamente entre $1.49$ y $2.63$, lo que permite inferir el rango razonable de $\\epsilon$ para evaluar densidades consistentes. La nota incluida (`nota`) aclara que la silueta de DBSCAN se computa excluyendo los puntos clasificados como ruido, es decir, aquellos asignados al cl√∫ster $0$.\n",
    "\n",
    "Dentro del apartado **`tablas`**, el subelemento sil_methods presenta las anchuras medias de silueta para **k-means, GMM y Spectral Clustering** para $k = 2,\\dots,10$. Los valores muestran que los tres m√©todos alcanzan sus puntuaciones m√°s altas alrededor de $k = 4‚Äì6$, especialmente k-means con un m√°ximo en $k = 6$ (`sil_kmeans = 0.2708`), seguido muy de cerca por GMM y, en menor medida, por Spectral Clustering. Este comportamiento sugiere que, desde la m√©trica de silueta, los datos presentan una estructura moderadamente definida en ese rango de particiones, con cierta p√©rdida de coherencia para valores mayores de $k$.\n",
    "\n",
    "Finalmente, el subelemento `dbscan` resume la evoluci√≥n de DBSCAN para distintos valores de $\\epsilon$. Se observa que todas las configuraciones entre $0.40$ y $1.15$ producen **0 cl√∫steres y 18 puntos de ruido**, lo que implica que con esos radios no se alcanza suficiente densidad para formar agrupamientos estables (`n_clusters`, `n_noise`). S√≥lo en $\\epsilon = 1.20$ el algoritmo identifica **2 cl√∫steres** con **11 puntos de ruido**, alcanzando adem√°s una **silueta media elevada** (`avg_sil_clu = 0.44097`), lo que indica que, aunque DBSCAN es muy sensible al par√°metro $\\epsilon$, existe al menos una configuraci√≥n que revela una estructura de densidad coherente en los datos.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos los resultados de los 4 m√©todos no jer√°rquicos\n",
    "# Extraemos la tabla de la Silueta Promedio\n",
    "tabla_k_silueta = res_silueta$tablas$sil_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a27d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la tabla comparativa de los m√©todos k-means, DBSCAN y GMM (Silueta promedio)\n",
    "kable(tabla_k_silueta, caption = \"Resumen de √≠ndices para k-means, GMM y Spectral Clustering (Silueta promedio)\", digits = 3, align = \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27a086",
   "metadata": {},
   "source": [
    "Antes de realizar la interpretaci√≥n cabe resaltar que la interpretaci√≥n del valor ser√° la <u>regla emp√≠rica com√∫nmente aceptada</u> la cual es:\n",
    "\n",
    "- Si la silueta promedio est√° entre $0.71$ y $1$ entonces la estructura de clasificaci√≥n encontrada es fuerte.\n",
    "\n",
    "- Si la silueta promedio est√° entre $0.51$ y $0.7$ entonces la estructura de clasificaci√≥n encontrada es razonable (aceptable).\n",
    "\n",
    "- Si la silueta promedio est√° entre $0.26$ y $0.50$ entonces la estructura de clasificaci√≥n encontrada es d√©bil y puede ser artificial.\n",
    "\n",
    "- Si la silueta promedio es $< 0.25$ entonces no se ha encontrado una estructura de clasificaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c1346",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** La tabla recoge las siluetas promedio de **k-means**, **GMM** y **Spectral Clustering** para $k = 2,\\dots,10$, permitiendo evaluar la calidad relativa de cada partici√≥n. En primer lugar, se observa que los valores m√°ximos se concentran en $k=6$, lo que indica que la estructura de los datos es moderada. El mayor valor de toda la tabla corresponde a **K-means** (`sil_kmeans = 0.271`), aunque este valor se encuentra por minimamente del umbral m√≠nimo de $0.25$, por lo que no constituye evidencia suficiente de una estructura fuerte.\n",
    "\n",
    "En cuanto a **Spectral Clustering** y **GMM**, ambos m√©todos muestran un comportamiento muy similar: las siluetas aumentan gradualmente hasta alcanzar valores moderados en **$k=4$** (`0.239` en Spectral Clustering y `0.246` en GMM) y alcanzan su m√°ximo en **$k=6$** (`0.247` y `0.255`, respectivamente). Sin embargo, incluso estos m√°ximos permanecen dentro del rango de **estructura no encontrada**, lo que indica que la partici√≥n solo est√° respaldada de manera tenue por la forma geom√©trica de los datos. A partir de ah√≠, todos los m√©todos muestran una degradaci√≥n progresiva.\n",
    "\n",
    "En conjunto, aunque ninguno de los m√©todos obtiene valores de Silueta que indiquen **estructura fuerte** ni **razonable**, se aprecia que los valores m√°s consistentes y altos se concentran entre **$k = 4$ y $k = 6$**, siendo **k-means y GMM con $k=6$** los que alcanzan los mejores resultados relativos. Por tanto, estos an√°lisis apuntan a que la estructura de los datos es **d√©bil** pero m√°s estable en torno a ese rango.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fe2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la tabla comparativa del m√©todo DBSCAN (Silueta promedio)\n",
    "kable(res_silueta$tablas$dbscan, caption = \"Resumen de resultados para DBSCAN seg√∫n eps (Silueta promedio)\", digits = 3, align = \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d5a89",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** La tabla resume c√≥mo evoluciona el comportamiento de **DBSCAN** al variar el par√°metro $\\epsilon$ manteniendo `minPts = 3`. Se observa que para todos los valores comprendidos entre `eps = 0.40` y `eps = 1.15`, el algoritmo detecta **0 cl√∫steres** (`n_clusters = 0`) y clasifica a **las 18 observaciones como ruido** (`n_noise = 18`). En consecuencia, la silueta promedio no puede calcularse (`avg_sil_clu = NA`), ya que DBSCAN no llega a formar ning√∫n grupo v√°lido. Este comportamiento indica que, en ese rango de vecindad, los puntos nunca alcanzan una densidad suficiente como para generar cl√∫steres estables.\n",
    "\n",
    "La √∫nica excepci√≥n se produce en `eps = 1.20`, donde el algoritmo identifica **2 cl√∫steres** (`n_clusters = 2`) y reduce el n√∫mero de puntos ruido a **11** (`n_noise = 11`). En este caso s√≠ es posible calcular la silueta, obteni√©ndose un valor de **0.441** (`avg_sil_clu = 0.4409738`), lo que seg√∫n las reglas emp√≠ricas corresponde a una **estructura de cl√∫steres d√©bil pero existente**. Este resultado pone de manifiesto que solo con valores de $\\epsilon$ considerablemente grandes DBSCAN consigue detectar alguna estructura en los datos, aunque dicha estructura no alcanza niveles altos de separaci√≥n o compacidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a340f",
   "metadata": {},
   "source": [
    "Se proceder√° a graficar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la tabla de resultados de Silueta\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "tabla_sil = res_silueta$tablas$sil_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c795f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pasamos a formato largo\n",
    "tabla_sil_long = tabla_sil %>%\n",
    "  pivot_longer(\n",
    "    cols      = c(sil_kmeans, sil_gmm, sil_spectral),\n",
    "    names_to  = \"metodo\",\n",
    "    values_to = \"silueta\"\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    metodo = recode(\n",
    "      metodo,\n",
    "      \"sil_kmeans\"   = \"k-means (Silueta)\",\n",
    "      \"sil_gmm\"      = \"GMM (Silueta)\",\n",
    "      \"sil_spectral\" = \"Spectral (Silueta)\"\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd26359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos gr√°ficamente la Silueta Promedio\n",
    "ggplot(tabla_sil_long, aes(x = k, y = silueta, color = metodo)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 2) +\n",
    "  labs(title = \"Silueta Promedio para k-means, GMM y Spectral Clustering\",\n",
    "       subtitle = \"Valores cercanos a 1 indican cl√∫steres densos y bien separados\",\n",
    "       x     = \"N√∫mero de cl√∫steres (k)\",\n",
    "       y     = \"Anchura de Silueta Promedio\",\n",
    "       color = \"M√©todo\") +\n",
    "  scale_x_continuous(breaks = tabla_sil$k) +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810865ba",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** El an√°lisis de la **silueta promedio** indica que el n√∫mero √≥ptimo de cl√∫steres se alcanza en **k = 6**, donde los tres algoritmos logran sus valores m√°ximos de separaci√≥n entre grupos y cohesi√≥n interna. En este punto, **k-means** obtiene la mayor calidad de partici√≥n con una silueta de **0.271**, seguido de **GMM** con **0.255**, y **Spectral Clustering** con **0.247**, lo que sugiere que, aunque las diferencias no son grandes, k-means produce los cl√∫steres m√°s compactos y mejor definidos para este valor de k.\n",
    "\n",
    "A partir de $k > 6$, las curvas de los tres m√©todos muestran un descenso progresivo en la calidad de las soluciones, indicando que a√±adir m√°s cl√∫steres produce agrupamientos menos cohesionados y peor separados. Por el contrario, para valores menores de k, la estructura de los datos a√∫n no est√° completamente capturada y la silueta es inferior en todos los casos. La convergencia de los m√°ximos en **k = 6** refuerza que este es el punto donde la partici√≥n ofrece el equilibrio m√°s robusto entre complejidad del modelo y calidad del agrupamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la tabla DBSCAN del objeto res_silueta\n",
    "tabla_sil_db = res_silueta$tablas$dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico espec√≠fico para Silueta en DBSCAN\n",
    "# Nota: Los NAs (donde no hubo suficientes clusters) no se pintar√°n, cortando la l√≠nea,\n",
    "# lo cual es correcto para indicar que ah√≠ no hay soluci√≥n v√°lida.\n",
    "ggplot(tabla_sil_db, aes(x = eps, y = avg_sil_clu)) +\n",
    "  geom_line(linewidth = 0.8, color = \"purple\") +\n",
    "  geom_point(size = 2, color = \"purple\") +\n",
    "  labs(title = \"DBSCAN: Calidad de los cl√∫steres (Silueta) seg√∫n eps\",\n",
    "       subtitle = \"Calculado excluyendo el ruido. Espacios vac√≠os indican < 2 cl√∫steres.\",\n",
    "       x     = \"eps (Radio de vecindad)\",\n",
    "       y     = \"Silueta Promedio (sin ruido)\") +\n",
    "  scale_x_continuous(breaks = tabla_sil_db$eps) +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591e207",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-**  \n",
    "El gr√°fico muestra la evoluci√≥n del n√∫mero de cl√∫steres detectados por **DBSCAN** en funci√≥n del par√°metro `eps`. Para un amplio rango inicial (`eps` entre 0.40 y 1.15), el algoritmo devuelve **cero cl√∫steres v√°lidos**, lo que significa que **todos los puntos son considerados ruido** bajo estos niveles de exigencia de densidad. Esto sugiere que, con `minPts = 3`, la distancia entre puntos es demasiado grande para formar n√∫cleos densos en ese intervalo.\n",
    "\n",
    "Solo cuando `eps` alcanza valores alrededor de **1.20** aparece finalmente una estructura con **dos cl√∫steres**, lo que indica que el algoritmo empieza a considerar suficientes puntos dentro del radio para formar regiones densas coherentes.  \n",
    "\n",
    "En conjunto, el gr√°fico evidencia que el conjunto de datos **no presenta densidades suficientemente compactas** como para que DBSCAN detecte cl√∫steres en eps peque√±os o moderados. La aparici√≥n de cl√∫steres √∫nicamente en eps grandes sugiere que la estructura es **poco densa** y requiere radios amplios para conectarse, por lo que la configuraci√≥n de DBSCAN debe interpretarse con cautela en este caso.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85891fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evoluci√≥n del ruido seg√∫n eps\n",
    "ggplot(tabla_sil_db, aes(x = eps, y = n_noise)) +\n",
    "  geom_line(linewidth = 0.8, color = \"purple\") +\n",
    "  geom_point(size = 2, color = \"purple\") +\n",
    "  labs(title = \"DBSCAN: Cantidad de ruido seg√∫n eps\",\n",
    "       x     = \"eps (Radio de vecindad)\",\n",
    "       y     = \"Puntos considerados ruido\") +\n",
    "  scale_x_continuous(breaks = tabla_sil_db$eps) +\n",
    "  theme_minimal(base_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed2ce6",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-**  \n",
    "La gr√°fica muestra la evoluci√≥n del n√∫mero de puntos etiquetados como **ruido** por DBSCAN en funci√≥n del par√°metro `eps`. Para todo el rango inicial (`eps` entre 0.40 y 1.15), el algoritmo considera **a las 18 observaciones como ruido**, lo cual indica que **no existe suficiente densidad local** para formar cl√∫steres con `minPts = 3` en ese intervalo.\n",
    "\n",
    "Solo cuando `eps` alcanza aproximadamente **1.20** el n√∫mero de puntos ruido cae bruscamente hasta **11**, lo que significa que algunos puntos empiezan a encontrarse dentro de radios suficientemente amplios como para cumplir el criterio de densidad y formar un par de cl√∫steres v√°lidos.\n",
    "\n",
    "En conjunto, el gr√°fico confirma que el conjunto de datos presenta una **densidad muy baja o muy dispersa**, de modo que DBSCAN no identifica regiones densas hasta valores de `eps` relativamente grandes. Esto refuerza la idea de que, para este dataset, DBSCAN solo ofrece soluciones interpretables cuando se permite un radio amplio que conecte los puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad77e5",
   "metadata": {},
   "source": [
    "En este apartado se a√±adir√°n las siluetas individuales por medio de un **gr√°fico de siluetas**, dicho gr√°fico  muestra qu√© tan bien se **ajusta cada observaci√≥n** al **cl√∫ster** que ha sido **asignado comparando qu√© tan cerca** se **encuentra** de las **dem√°s en su cluster**.Se realizar√° para los $k=6$ y $eps=1.20$ ya que han sido los valores √≥ptimos.\n",
    "\n",
    "De cada uno se realizar√°n dos representaciones aunque ambas son equivalentes se realizan para ayudar en la comprensi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librer√≠as necesarias\n",
    "library(cluster) # Contiene la funci√≥n silhouette()\n",
    "install.packages(\"factoextra\")\n",
    "library(factoextra) # √ötil para la funci√≥n fviz_silhouette (m√°s est√©tica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignaciones de cl√∫steres\n",
    "# k=6 es el valor que utilizaremos para k-means, GMM y Spectral\n",
    "# Para DBSCAN, usamos la soluci√≥n robusta de eps=1.15\n",
    "\n",
    "# 1. k-means\n",
    "set.seed(123) # Para reproducibilidad\n",
    "res_kmeans_k3 = kmeans(datos_est, centers = 6, nstart = 25)\n",
    "k_means_clusters = res_kmeans_k3$cluster\n",
    "\n",
    "# 2. GMM\n",
    "res_gmm_k3 = Mclust(datos_est, G = 6)\n",
    "gmm_clusters = res_gmm_k3$classification\n",
    "\n",
    "# 3. DBSCAN\n",
    "res_dbscan_eps115 = dbscan(datos_est, eps = 1.2, minPts = 3)\n",
    "dbscan_clusters = res_dbscan_eps115$cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbaea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se necesita la matriz de asignaci√≥n y la matriz de distancias\n",
    "sil_kmeans = silhouette(k_means_clusters, datos_dist)\n",
    "plot(sil_kmeans, main = \"Silueta (k-means, k=6)\", border = NA, col = 2:(3+1)) # colorea hasta k+1 cl√∫steres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4488af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa para que teng√°is otro gr√°fico similar (requiere factoextra)\n",
    "fviz_silhouette(sil_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b13d0",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** La silueta promedio del modelo ($0.27$) revela que la estructura global de los cl√∫steres es **d√©bil**, indicando que las separaciones entre grupos son modestas y que existe una proximidad considerable entre observaciones de cl√∫steres distintos. Aun as√≠, el an√°lisis permite identificar patrones diferenciados en la calidad interna de cada cl√∫ster. En concreto, los cl√∫steres **1**, **4** y **5** muestran siluetas medias entre $0.31$ y $0.35$, lo que sugiere una cohesi√≥n razonable y una separaci√≥n algo m√°s clara respecto al resto. El cl√∫ster **5**, con una silueta de $0.35$, es el mejor definido dentro del modelo.\n",
    "\n",
    "Por el contrario, los cl√∫steres **3** y **6** presentan valores muy bajos de silueta ($0.10$ y $0.12$), indicando una estructura claramente d√©bil: muchas de sus observaciones est√°n m√°s cerca de otros cl√∫steres que del suyo propio. El cl√∫ster **2** ocupa una posici√≥n intermedia, con una silueta de $0.28$ que refleja una separaci√≥n moderada pero no s√≥lida. En conjunto, aunque el modelo logra identificar algunos grupos con cierta coherencia interna, la baja silueta promedio sugiere que la partici√≥n en seis cl√∫steres no captura una estructura de agrupamiento fuerte en los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos y graficamos siluetas para GMM (k=3)\n",
    "sil_gmm = silhouette(gmm_clusters, datos_dist)\n",
    "plot(sil_gmm, main = \"Silueta (GMM, k=6)\", border = NA, col = 2:(3+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa para que teng√°is otro gr√°fico similar (requiere factoextra)\n",
    "fviz_silhouette(sil_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5ef7f",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** La silueta promedio del modelo ($0.27$) revela que la estructura global de los cl√∫steres es **d√©bil**, indicando que las separaciones entre grupos son modestas y que existe una proximidad considerable entre observaciones de cl√∫steres distintos. Aun as√≠, el an√°lisis permite identificar patrones diferenciados en la calidad interna de cada cl√∫ster. En concreto, los cl√∫steres **1**, **2** y **5** muestran siluetas medias de $0.31$ y $0.32$, lo que sugiere una cohesi√≥n razonable y una separaci√≥n algo m√°s clara respecto al resto. Los cl√∫ster **2 y 1**, con una silueta de $0.32$, es los mejores definidos dentro del modelo.\n",
    "\n",
    "Por el contrario, los cl√∫steres **3** y **6** presentan valores muy bajos de silueta ($0.24$ y $0.12$), indicando una estructura claramente d√©bil: muchas de sus observaciones est√°n m√°s cerca de otros cl√∫steres que del suyo propio. El cl√∫ster **4** es un cluster unitario. En conjunto, aunque el modelo logra identificar algunos grupos con cierta coherencia interna, la baja silueta promedio sugiere que la partici√≥n en seis cl√∫steres no captura una estructura de agrupamiento fuerte en los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos y graficamos siluetas para DBSCAN (eps=1.15)\n",
    "# Importante: La funci√≥n silhouette trata al cl√∫ster 0 de DBSCAN (ruido) como un cl√∫ster separado.\n",
    "sil_dbscan = silhouette(dbscan_clusters, datos_dist)\n",
    "plot(sil_dbscan, main = \"Silueta (DBSCAN, eps=1.20)\", border = NA, col = 2:(max(dbscan_clusters)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa para que teng√°is otro gr√°fico similar (requiere factoextra)\n",
    "fviz_silhouette(sil_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e406cd",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n.-** La silueta promedio del modelo ($0.07$) revela que la estructura global de los cl√∫steres ***no se ha encontrado**, indicando que las separaciones entre grupos son grandes y que no existe una proximidad considerable entre observaciones de cl√∫steres distintos. Aun as√≠, el an√°lisis permite identificar patrones diferenciados en la calidad interna de cada cl√∫ster. En concreto, los cl√∫steres **1** y **2** muestran siluetas medias entre $0.36$ y $0.53$, lo que sugiere una cohesi√≥n razonable y una separaci√≥n algo m√°s clara respecto al resto. El cl√∫ster **2**, con una silueta de $0.53$, es el mejor definido dentro del modelo y siendo el √∫nico que posee una estructura aceptable entre todos los modelos.\n",
    "\n",
    "Por el contrario, el cluster $0$ posee un valor negativo, esto tiene sentido ya que es el formado por los valores outliers.  En conjunto, aunque el modelo logra identificar algunos grupos con cierta coherencia, la baja silueta promedio sugiere que la partici√≥n en tres cl√∫steres no captura una estructura de agrupamiento fuerte en los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2654f",
   "metadata": {},
   "source": [
    "**Conclusi√≥n.-** No se ha podido indentificar un modelo √≥ptimo debido a que entre los criterios no hay ni unidad ni mayor√≠a salbo en el dbsacn el cual nos indica que hay que dividirlo en 10 clusters pero pese a esa unidad los valores de los estad√≠sticos son malos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
